# References

<div id="refs" class="references">

<div id="ref-alder12">

\[1\] A. Adler, V. Emiya, M. G. Jafari, M. Elad, R. Gribonval, and M. D.
Plumbley, “Audio inpainting,” *IEEE Transactions on Audio, Speech, and
Language Processing*, vol. 20, no. 3, pp. 922–932, Mar. 2012.

</div>

<div id="ref-anden14">

\[2\] J. Andén and S. Mallat, “Deep scattering spectrum,” *IEEE
Transactions on Signal Processing*, vol. 62, no. 16, pp. 4114–4128, Aug.
2014.

</div>

<div id="ref-araki12">

\[3\] S. Araki *et al.*, “The 2011 signal separation evaluation campaign
(SiSEC2011): - audio source separation -,” in *10th international
conference on latent variable analysis and signal separation*, 2012.

</div>

<div id="ref-araki10">

\[4\] S. Araki *et al.*, “The 2010 signal separation evaluation campaign
(SiSEC2010): - audio source separation -,” in *9th international
conference on latent variable analysis and signal separation*, 2010.

</div>

<div id="ref-araki102">

\[5\] S. Araki *et al.*, “The 2010 signal separation evaluation campaign
(SiSEC2010): - biomedical source separation -,” in *9th international
conference on latent variable analysis and signal separation*, 2010.

</div>

<div id="ref-arberet10">

\[6\] S. Arberet, A. Ozerov, R. Gribonval, and F. Bimbot, “A robust
method to count and locate audio sources in a multichannel
underdetermined mixture,” *IEEE Transactions on Audio, Speech, and
Language Processing*, vol. 58, 2010.

</div>

<div id="ref-arberet09">

\[7\] S. Arberet, A. Ozerov, R. Gribonval, and F. Bimbot, “Blind
spectral-GMM estimation for underdetermined instantaneous audio source
separation,” in *8th international conference on independent component
analysis and signal separation*, 2009.

</div>

<div id="ref-avendano03">

\[8\] C. Avendano, “Frequency-domain source identification and
manipulation in stereo mixes for enhancement, suppression and re-panning
applications,” in *IEEE workshop on applications of signal processing to
audio and acoustics*, 2003.

</div>

<div id="ref-avendano02">

\[9\] C. Avendano and J.-M. Jot, “Frequency domain techniques for stereo
to multichannel upmix,” in *AES 22nd international conference*, 2002.

</div>

<div id="ref-barker15">

\[10\] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, “The third
‘CHiME’ speech separation and recognition challenge: Dataset, task and
baselines,” in *IEEE workshop on automatic speech recognition and
understanding*, 2015.

</div>

<div id="ref-barry04">

\[11\] D. Barry, B. Lawlor, and E. Coyle, “Sound source separation:
Azimuth discrimination and resynthesis,” in *7th international
conference on digital audio effects*, 2004.

</div>

<div id="ref-benaroya03">

\[12\] L. Benaroya and F. Bimbot, “Wiener based source separation with
HMM/GMM using a single sensor,” in *4th international symposium on
independent component analysis and blind signal separation*, 2003.

</div>

<div id="ref-benaroya06">

\[13\] L. Benaroya, F. Bimbot, and R. Gribonval, “Audio source
separation with a single sensor,” *IEEE Transactions on Audio, Speech,
and Language Processing*, vol. 14, no. 1, pp. 191–199, Jan. 2006.

</div>

<div id="ref-benaroya032">

\[14\] L. Benaroya, L. Mcdonagh, F. Bimbot, and R. Gribonval, “Non
negative sparse representation for Wiener based source separation with a
single sensor,” in *IEEE international conference on acoustics, speech
and signal processing*, 2003.

</div>

<div id="ref-ben-shalom04">

\[15\] A. Ben-Shalom and S. Dubnov, “Optimal filtering of an instrument
sound in a mixed recording given approximate pitch prior,” in
*International computer music conference*, 2004.

</div>

<div id="ref-berenzweig01">

\[16\] A. L. Berenzweig and D. P. Ellis, “Locating singing voice
segments within music signals,” in *IEEE workshop on applications of
signal processing to audio and acoustics*, 2001.

</div>

<div id="ref-bernard01">

\[17\] C. P. Bernard, “Discrete wavelet analysis for fast optic flow
computation,” *Applied and Computational Harmonic Analysis*, vol. 11,
no. 1, pp. 32–63, Jul. 2001.

</div>

<div id="ref-bishop96">

\[18\] C. Bishop, *Neural networks for pattern recognition*. Clarendon
Press, 1996.

</div>

<div id="ref-bittner14">

\[19\] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam, and and
Juan P. Bello, “MedleyDB: A multitrack dataset for annotation-intensive
mir research,” in *15th international society for music information
retrieval conference*, 2014.

</div>

<div id="ref-bittner17">

\[20\] R. Bittner, A. Wang, and J. P. Bello, “Pitch contour tracking in
music using harmonic locked loops,” in *IEEE international conference on
acoustics, speech and signal processing*, 2017.

</div>

<div id="ref-bogert1963">

\[21\] B. P. Bogert, M. J. R. Healy, and J. W. Tukey, “The quefrency
alanysis of time series for echoes: Cepstrum pseudo-autocovariance,
cross-cepstrum, and saphe cracking,” *Proceedings of a symposium on time
series analysis*, pp. 209–243, 1963.

</div>

<div id="ref-boersma01">

\[22\] P. Boersma, “PRAAT, a system for doing phonetics by computer,”
*Glot International*, vol. 5, no. 9/10, pp. 341–347, Dec. 2001.

</div>

<div id="ref-boersma93">

\[23\] P. Boersma, “Accurate short-term analysis of the fundamental
frequency and the harmonics-to-noise ratio of a sampled sound,” in *IFA
proceedings 17*, 1993.

</div>

<div id="ref-boll1979">

\[24\] S. Boll, “Suppression of acoustic noise in speech using spectral
subtraction,” *IEEE Transactions on acoustics, speech, and signal
processing*, vol. 27, no. 2, pp. 113–120, 1979.

</div>

<div id="ref-bosch12">

\[25\] J. J. Bosch, K. Kondo, R. Marxer, and J. Janer, “Score-informed
and timbre independent lead instrument separation in real-world
scenarios,” in *20th european signal processing conference*, 2012.

</div>

<div id="ref-boulanger-lewandowski14">

\[26\] N. Boulanger-Lewandowski, G. J. Mysore, and M. Hoffman,
“Exploiting long-term temporal dependencies in NMF using recurrent
neural networks with application to source separation,” in *IEEE
international conference on acoustics, speech and signal processing*,
2014.

</div>

<div id="ref-bregman90">

\[27\] A. S. Bregman, *Auditory scene analysis*. MIT Press, 1990.

</div>

<div id="ref-breiman84">

\[28\] L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen,
*Classification and regression trees*. Chapman; Hall/CRC, 1984.

</div>

<div id="ref-brown91">

\[29\] J. C. Brown, “Calculation of a constant Q spectral transform,”
*Journal of the Acoustical Society of America*, vol. 89, no. 1, pp.
425–434, Jan. 1991.

</div>

<div id="ref-brown92">

\[30\] J. C. Brown and M. S. Puckette, “An efficient algorithm for the
calculation of a constant Q transform,” *Journal of the Acoustical
Society of America*, vol. 92, no. 5, pp. 2698–2701, Nov. 1992.

</div>

<div id="ref-bryan13">

\[31\] N. J. Bryan and G. J. Mysore, “Interactive user-feedback for
sound source separation,” in *International conference on intelligent
user-interfaces, workshop on interactive machine learning*, 2013.

</div>

<div id="ref-bryan132">

\[32\] N. J. Bryan and G. J. Mysore, “An efficient posterior regularized
latent variable model for interactive sound source separation,” in *30th
international conference on machine learning*, 2013.

</div>

<div id="ref-bryan133">

\[33\] N. J. Bryan and G. J. Mysore, “Interactive refinement of
supervised and semi-supervised sound source separation estimates,” in
*IEEE international conference on acoustics, speech, and signal
processing*, 2013.

</div>

<div id="ref-buades05">

\[34\] A. Buades, B. Coll, and J.-M. Morel, “A non-local algorithm for
image denoising,” in *Computer society conference on computer vision and
pattern recognition*, 2005.

</div>

<div id="ref-candes11">

\[35\] E. J. Candès, X. Li, Y. Ma, and J. Wright, “Robust principal
component analysis?” *Journal of the ACM*, vol. 58, no. 3, pp. 1–37, May
2011.

</div>

<div id="ref-cano09">

\[36\] E. Cano and C. Cheng, “Melody line detection and source
separation in classical saxophone recordings,” in *12th international
conference on digital audio effects*, 2009.

</div>

<div id="ref-cano13">

\[37\] E. Cano, C. Dittmar, and G. Schuller, “Re-thinking sound
separation: Prior information and additivity constraints in separation
algorithms,” in *16th international conference on digital audio
effects*, 2013.

</div>

<div id="ref-cano12">

\[38\] E. Cano, C. Dittmar, and G. Schuller, “Efficient implementation
of a system for solo and accompaniment separation in polyphonic music,”
in *20th european signal processing conference*, 2012.

</div>

<div id="ref-cano11">

\[39\] E. Cano, C. Dittmar, and G. Schuller, “Influence of phase,
magnitude and location of harmonic components in the perceived quality
of extracted solo signals,” in *AES 42nd conference on semantic audio*,
2011.

</div>

<div id="ref-cano14">

\[40\] E. Cano, G. Schuller, and C. Dittmar, “Pitch-informed solo and
accompaniment separation towards its use in music education
applications,” *EURASIP Journal on Advances in Signal Processing*, vol.
2014, no. 23, Sep. 2014.

</div>

<div id="ref-cano10">

\[41\] E. Cano, G. Schuller, and C. Dittmar, “Exploring phase
information in sound source separation applications,” in *13th
international conference on digital audio effects*, 2010.

</div>

<div id="ref-cappe2005">

\[42\] O. Cappé, E. Moulines, and T. Ryden, *Inference in hidden markov
models (springer series in statistics)*. Secaucus, NJ, USA:
Springer-Verlag New York, Inc., 2005.

</div>

<div id="ref-cartwright16">

\[43\] M. Cartwright, B. Pardo, G. J. Mysore, and M. Hoffman, “Fast and
easy crowdsourced perceptual audio evaluation,” in *IEEE international
conference on acoustics, speech and signal processing*, 2016.

</div>

<div id="ref-casey00">

\[44\] M. A. Casey and A. Westner, “Separation of mixed audio sources by
independent subspace analysis,” in *International computer music
conference*, 2000.

</div>

<div id="ref-cemgil09">

\[45\] A. T. Cemgil, “Bayesian inference for nonnegative matrix
factorisation models,” *Computational Intelligence and Neuroscience*,
vol. 2009, no. 4, pp. 1–17, Jan. 2009.

</div>

<div id="ref-chan15">

\[46\] T.-S. Chan *et al.*, “Vocal activity informed singing voice
separation with the iKala dataset,” in *IEEE international conference on
acoustics, speech and signal processing*, 2015.

</div>

<div id="ref-chan17">

\[47\] T.-S. T. Chan and Y.-H. Yang, “Informed group-sparse
representation for singing voice separation,” *IEEE Signal Processing
Letters*, vol. 24, no. 2, pp. 156–160, Feb. 2017.

</div>

<div id="ref-chan16">

\[48\] T.-S. T. Chan and Y.-H. Yang, “Complex and quaternionic principal
component pursuit and its application to audio separation,” *IEEE Signal
Processing Letters*, vol. 23, no. 2, pp. 287–291, Feb. 2016.

</div>

<div id="ref-chandna17">

\[49\] P. Chandna, M. Miron, J. Janer, and E. Gómez, “Monoaural audio
source separation using deep convolutional neural networks,” in *13th
international conference on latent variable analysis and signal
separation*, 2017.

</div>

<div id="ref-chanrungutai08">

\[50\] A. Chanrungutai and C. A. Ratanamahatana, “Singing voice
separation for mono-channel music using non-negative matrix
factorization,” in *International conference on advanced technologies
for communications*, 2008.

</div>

<div id="ref-chanrungutai082">

\[51\] A. Chanrungutai and C. A. Ratanamahatana, “Singing voice
separation in mono-channel music,” in *International symposium on
communications and information technologies*, 2008.

</div>

<div id="ref-chen01">

\[52\] B. Chen and G. W. Wornell, “Quantization index modulation: A
class of provably good methods for digital watermarking and information
embedding,” *IEEE Transactions on Information Theory*, vol. 47, 2001.

</div>

<div id="ref-decheveigne02">

\[53\] A. de Cheveigné and H. Kawahara, “YIN, a fundamental frequency
estimator for speech and music,” *Journal of the Acoustical Society of
America*, vol. 111, no. 4, pp. 1917–1930, Apr. 2002.

</div>

<div id="ref-chi99">

\[54\] T. Chi, Y. Gao, M. C. Guyton, P. Ru, and S. Shamma,
“Spectro-temporal modulation transfer functions and speech
intelligibility,” *Journal of the Acoustical Society of America*, vol.
106, no. 5, pp. 2719–2732, Nov. 1999.

</div>

<div id="ref-chi05">

\[55\] T. Chi, P. Rub, and S. A. Shamma, “Multiresolution
spectrotemporal analysis of complex sounds,” *Journal of the Acoustical
Society of America*, vol. 118, no. 2, pp. 887–906, Aug. 2005.

</div>

<div id="ref-chien15">

\[56\] J.-T. Chien and P.-K. Yang, “Bayesian factorization and learning
for monaural source separation,” *IEEE/ACM Transactions on Audio,
Speech, and Language Processing*, vol. 24, no. 1, pp. 185–195, Jan.
2015.

</div>

<div id="ref-cho15">

\[57\] H.-S. Cho, J.-Y. Lee, and H.-G. Kim, “Singing voice separation
from monaural music based on kernel back-fitting using beta-order
spectral amplitude estimation,” in *16th international society for music
information retrieval conference*, 2015.

</div>

<div id="ref-cichocki09">

\[58\] A. Cichocki, R. Zdunek, A. H. Phan, and S.-i. Amari, *Nonnegative
matrix and tensor factorizations*. Wiley, 2009.

</div>

<div id="ref-cobos083">

\[59\] M. Cobos and J. J. López, “Improving isolation of blindly
separated sources using time-frequency masking,” *IEEE Signal Processing
Letters*, vol. 15, 2008.

</div>

<div id="ref-cobos08">

\[60\] M. Cobos and J. J. López, “Singing voice separation combining
panning information and pitch tracking,” in *AES 124th convention*,
2008.

</div>

<div id="ref-cobos082">

\[61\] M. Cobos and J. J. López, “Stereo audio source separation based
on time-frequency masking and multilevel thresholding,” *Digital Signal
Processing*, vol. 18, no. 6, pp. 960–976, Nov. 2008.

</div>

<div id="ref-common94">

\[62\] P. Common, “Independent component analysis, a new concept?”
*Signal Processing*, vol. 36, no. 3, pp. 287–314, Apr. 1994.

</div>

<div id="ref-common10">

\[63\] P. Common and C. Jutten, *Handbook of blind source separation*.
Academic Press, 2010.

</div>

<div id="ref-creager16">

\[64\] E. Creager, N. D. Stein, R. Badeau, and P. Depalle, “Nonnegative
tensor factorization with frequency modulation cues for blind audio
source separation,” in *17th international society for music information
retrieval conference*, 2016.

</div>

<div id="ref-dannenberg08">

\[65\] R. B. Dannenberg and M. Goto, “Music structure analysis from
acoustic signals,” in *Handbook of signal processing in acoustics*,
Springer New York, 2008, pp. 305–331.

</div>

<div id="ref-david80">

\[66\] S. B. Davis and P. Mermelstein, “Comparison of parametric
representations for monosyllabic word recognition in continuously spoken
sentences,” *IEEE Transactions on Audio, Speech, and Language
Processing*, vol. 28, no. 4, pp. 357–366, Aug. 1980.

</div>

<div id="ref-degottex11">

\[67\] G. Degottex, A. Roebel, and X. Rodet, “Pitch transposition and
breathiness modification using a glottal source model and its adapted
vocal-tract filter,” in *IEEE international conference on acoustics,
speech and signal processing*, 2011.

</div>

<div id="ref-deif152">

\[68\] H. Deif, D. FitzGerald, W. Wang, and L. Gan, “Separation of
vocals from monaural music recordings using diagonal median filters and
practical time-frequency parameters,” in *IEEE international symposium
on signal processing and information technology*, 2015.

</div>

<div id="ref-deif15">

\[69\] H. Deif, W. Wang, L. Gan, and S. Alhashmi, “A local discontinuity
based approach for monaural singing voice separation from accompanying
music with multi-stage non-negative matrix factorization,” in *IEEE
global conference on signal and information processing*, 2015.

</div>

<div id="ref-dempster77">

\[70\] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood
from incomplete data via the EM algorithm,” *Journal of the Royal
Statistical Society*, vol. 39, no. 1, pp. 1–38, 1977.

</div>

<div id="ref-deng14">

\[71\] L. Deng and D. Yu, “Deep learning: Methods and applications,”
*Foundations and Trends in Signal Processing*, vol. 7, nos. 3-4, pp.
197–387, Jun. 2014.

</div>

<div id="ref-dhillon05">

\[72\] I. S. Dhillon and S. Sra, “Generalized nonnegative matrix
approximations with Bregman divergences,” in *Advances in neural
information processing systems 18*, MIT Press, 2005, pp. 283–290.

</div>

<div id="ref-ding97">

\[73\] Y. Ding and X. Qian, “Processing of musical tones using a
combined quadratic polynomial-phase sinusoid and residual (QUASAR)
signal model,” *Journal of the Audio Engineering Society*, vol. 45, no.
7/8, pp. 571–584, Jul. 1997.

</div>

<div id="ref-dittmar12">

\[74\] C. Dittmar, E. Cano, J. Abeßer, and S. Grollmisch, “Music
information retrieval meets music education,” in *Multimodal music
processing*, Dagstuhl Publishing, 2012, pp. 95–120.

</div>

<div id="ref-dobashi15">

\[75\] A. Dobashi, Y. Ikemiya, K. Itoyama, and K. Yoshii, “A music
performance assistance system based on vocal, harmonic, and percussive
source separation and content visualization for music audio signals,” in
*12th sound and music computing conference*, 2015.

</div>

<div id="ref-dressler11">

\[76\] K. Dressler, “Pitch estimation by the pair-wise evaluation of
spectral peaks,” in *42nd aes conference on semantic audio*, 2011.

</div>

<div id="ref-dressler06">

\[77\] K. Dressler, “An auditory streaming approach on melody
extraction,” in *7th international conference on music information
retrieval*, 2006.

</div>

<div id="ref-dressler062">

\[78\] K. Dressler, “Sinusoidal extraction using an efficient
implementation of a multi-resolution FFT,” in *9th international
conference on digital audio effects*, 2006.

</div>

<div id="ref-dressler112">

\[79\] K. Dressler, “An auditory streaming approach for melody
extraction from polyphonic music,” in *7th international society for
music information retrieval conference*, 2006.

</div>

<div id="ref-driedger14">

\[80\] J. Driedger, M. Müller, and S. Disch, “Extending
harmonic-percussive separation of audio signals,” in *15th international
society for music information retrieval conference*, 2014.

</div>

<div id="ref-driedger15">

\[81\] J. Driedger and M. Müller, “Extracting singing voice from music
recordings by cascading audio decomposition techniques,” in *IEEE
international conference on acoustics, speech and signal processing*,
2015.

</div>

<div id="ref-duan10">

\[82\] Z. Duan and B. Pardo, “Multiple fundamental frequency estimation
by modeling spectral peaks and non-peak regions,” *IEEE Transactions on
Audio, Speech, and Language Processing*, vol. 18, no. 8, pp. 2121–2133,
Nov. 2010.

</div>

<div id="ref-duan08">

\[83\] Z. Duan, Y.-F. Zhang, C.-S. Zhang, and Z. Shi, “Unsupervised
single-channel music source separation by average harmonic structure
modeling,” *IEEE Transactions on Audio, Speech, and Language
Processing*, vol. 16, no. 4, pp. 766–778, May 2008.

</div>

<div id="ref-duong10">

\[84\] N. Q. K. Duong, E. Vincent, and R. Gribonval, “Under-determined
reverberant audio source separation using a full-rank spatial covariance
model,” *IEEE Transactions on Audio, Speech, and Language Processing*,
vol. 18, no. 7, pp. 1830–1840, Sep. 2010.

</div>

<div id="ref-durrett2010probability">

\[85\] R. Durrett, *Probability: Theory and examples*. Cambridge
university press, 2010.

</div>

<div id="ref-durrieu11">

\[86\] J.-L. Durrieu, B. David, and G. Richard, “A musically motivated
mid-level representation for pitch estimation and musical audio source
separation,” *IEEE Journal of Selected Topics in Signal Processing*,
vol. 5, no. 6, pp. 1180–1191, Oct. 2011.

</div>

<div id="ref-durrieu092">

\[87\] J.-L. Durrieu, A. Ozerov, C. Févotte, G. Richard, and B. David,
“Main instrument separation from stereophonic audio signals using a
source/filter model,” in *17th european signal processing conference*,
2009.

</div>

<div id="ref-durrieu09">

\[88\] J.-L. Durrieu, G. Richard, and B. David, “An iterative approach
to monaural musical mixture de-soloing,” in *IEEE international
conference on acoustics, speech and signal processing*, 2009.

</div>

<div id="ref-durrieu08">

\[89\] J.-L. Durrieu, G. Richard, and B. David, “Singer melody
extraction in polyphonic signals using source separation methods,” in
*IEEE international conference on acoustics, speech and signal
processing*, 2008.

</div>

<div id="ref-durrieu10">

\[90\] J.-L. Durrieu, G. Richard, B. David, and C. Févotte,
“Source/filter model for unsupervised main melody extraction from
polyphonic audio signals,” *IEEE Transactions on Audio, Speech, and
Language Processing*, vol. 18, no. 3, pp. 564–575, Mar. 2010.

</div>

<div id="ref-durrieu12">

\[91\] J.-L. Durrieu and J.-P. Thiran, “Musical audio source separation
based on user-selected F0 track,” in *10th international conference on
latent variable analysis and signal separation*, 2012.

</div>

<div id="ref-duxbury03">

\[92\] C. Duxbury, J. P. Bello, M. Davies, and M. Sandler, “Complex
domain onset detection for musical signals,” in *6th international
conference on digital audio effects*, 2003.

</div>

<div id="ref-emiya11">

\[93\] V. Emiya, E. Vincent, N. Harlander, and V. Hohmann, “Subjective
and objective quality assessment of audio source separation,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 19, no. 7,
pp. 2046–2057, Sep. 2011.

</div>

<div id="ref-emiya10">

\[94\] V. Emiya, E. Vincent, N. Harlander, and V. Hohmann,
“Multi-criteria subjective and objective evaluation of audio source
separation,” in *38th international aes conference*, 2010.

</div>

<div id="ref-erdogan15">

\[95\] H. Erdogan, J. R. Hershey, S. Watanabe, and J. L. Roux,
“Phase-sensitive and recognition-boosted speech separation using deep
recurrent neural networks,” in *IEEE international conference on
acoustics, speech and signal processing*, 2015.

</div>

<div id="ref-ewert14">

\[96\] S. Ewert, B. Pardo, M. Müller, and M. D. Plumbley,
“Score-informed source separation for musical audio recordings: An
overview,” *IEEE Signal Processing Magazine*, vol. 31, no. 3, pp.
116–124, May 2014.

</div>

<div id="ref-fevotte09">

\[97\] C. Févotte, “Nonnegative matrix factorization with the
Itakura-Saito divergence: With application to music analysis,” *Neural
Computation*, vol. 21, no. 3, pp. 793–830, Mar. 2009.

</div>

<div id="ref-fevotte05">

\[98\] C. Févotte, R. Gribonval, and E. Vinvent, “BSS\_EVAL toolbox user
guide - revision 2.0,” IRISA, 2005.

</div>

<div id="ref-fevotte10">

\[99\] C. Févotte and A. Ozerov, “Notes on nonnegative tensor
factorization of the spectrogram for audio source separation:
Statistical insights and towards self-clustering of the spatial cues,”
in *7th international symposium on computer music modeling and
retrieval*, 2010.

</div>

<div id="ref-fan16">

\[100\] Z.-C. Fan, J.-S. R. Jang, and C.-L. Lu, “Singing voice
separation and pitch extraction from monaural polyphonic audio music via
DNN and adaptive pitch tracking,” in *IEEE international conference on
multimedia big data*, 2016.

</div>

<div id="ref-fant70">

\[101\] G. Fant, *Acoustic theory of speech production*. Walter de
Gruyter, 1970.

</div>

<div id="ref-feng02">

\[102\] Y. Feng, Y. Zhuang, and Y. Pan, “Popular music retrieval by
independent component analysis,” in *3rd international conference on
music information retrieval*, 2002.

</div>

<div id="ref-feng01">

\[103\] Y. Feng, Y. Zhuang, and Y. Pan, “Query similar music by
correlation degree,” in *IEEE pacific-rim conference on multimedia*,
2001.

</div>

<div id="ref-fitzgerald13">

\[104\] D. FitzGerald, “Stereo vocal extraction using ADRess and nearest
neighbours median filtering,” in *16th international conference on
digital audio effects*, 2013.

</div>

<div id="ref-fitzgerald12">

\[105\] D. FitzGerald, “Vocal separation using nearest neighbours and
median filtering,” in *23rd iet irish signals and systems conference*,
2012.

</div>

<div id="ref-fitzgerald10">

\[106\] D. FitzGerald, “Harmonic/percussive separation using median
filtering,” in *13th international conference on digital audio effects*,
2010.

</div>

<div id="ref-fitzgerald09">

\[107\] D. FitzGerald, M. Cranitch, and E. Coyle, “Using tensor
factorisation models to separate drums from polyphonic music,” in *12th
international conference on digital audio effects*, 2009.

</div>

<div id="ref-fitzgerald102">

\[108\] D. FitzGerald and M. Gainza, “Single channel vocal separation
using median filtering and factorisation techniques,” *ISAST
Transactions on Electronic and Signal Processing*, vol. 4, no. 1, pp.
62–73, Jan. 2010.

</div>

<div id="ref-fitzgerald132">

\[109\] D. FitzGerald and R. Jaiswal, “Improved stereo instrumental
track recovery using median nearest-neighbour inpainting,” in *24th iet
irish signals and systems conference*, 2013.

</div>

<div id="ref-fitzgerald14">

\[110\] D. FitzGerald, A. Liutkus, Z. Rafii, B. Pardo, and L. Daudet,
“Harmonic/percussive separation using kernel additive modelling,” in
*25th iet irish signals and systems conference*, 2014.

</div>

<div id="ref-foote99">

\[111\] J. Foote, “Visualizing music and audio using self-similarity,”
in *7th acm international conference on multimedia*, 1999.

</div>

<div id="ref-foote01">

\[112\] J. Foote and S. Uchihashi, “The beat spectrum: A new approach to
rhythm analysis,” in *IEEE international conference on multimedia and
expo*, 2001.

</div>

<div id="ref-fox072">

\[113\] B. Fox and B. Pardo, “Towards a model of perceived quality of
blind audio source separation,” in *IEEE international conference on
multimedia and expo*, 2007.

</div>

<div id="ref-fox07">

\[114\] B. Fox, A. Sabin, B. Pardo, and A. Zopf, “Modeling perceptual
similarity of audio signals for blind source separation evaluation,” in
*7th international conference on latent variable analysis and signal
separation*, 2007.

</div>

<div id="ref-fritsch12">

\[115\] J. Fritsch, “High quality musical audio source separation,”
Master’s thesis, UPMC / IRCAM / Telecom ParisTech, 2012.

</div>

<div id="ref-fuentes2012">

\[116\] B. Fuentes, R. Badeau, and G. Richard, “Blind harmonic adaptive
decomposition applied to supervised source separation,” in *Signal
processing conference (eusipco), 2012 proceedings of the 20th european*,
2012, pp. 2654–2658.

</div>

<div id="ref-fujihara10">

\[117\] H. Fujihara, M. Goto, T. Kitahara, and H. G. Okuno, “A modeling
of singing voice robust to accompaniment sounds and its application to
singer identification and vocal-timbre-similarity-based music
information retrieval,” *IEEE Transactions on Audio, Speech, and
Language Processing*, vol. 18, no. 3, pp. 638–648, Mar. 2010.

</div>

<div id="ref-fujihara05">

\[118\] H. Fujihara, T. Kitahara, M. Goto, K. Komatani, T. Ogata, and H.
G. Okuno, “Singer identification based on accompaniment sound reduction
and reliable frame selection,” in *6th international conference on music
information retrieval*, 2005.

</div>

<div id="ref-gomez12">

\[119\] E. Gómez, F. J. C. Quesada, J. Salamon, J. Bonada, P. V. Candea,
and P. C. Molero, “Predominant fundamental frequency estimation vs
singing voice separation for the automatic transcription of accompanied
flamenco singing,” in *13th international society for music information
retrieval conference*, 2012.

</div>

<div id="ref-gaikwad16">

\[120\] S. S. Gaikwad, P. P. Ingale, and S. L. Nalbalwar, “Separation of
singing voice from background musical noise using modified NMF and
filtering,” in *International conference on electrical, electronics, and
optimization techniques*, 2016.

</div>

<div id="ref-gales96">

\[121\] M. J. Gales, D. Pye, and P. C. Woodland, “Variance compensation
within the MLLR framework for robust speech recognition and speaker
adaptation,” in *4th international conference on spoken language
processing*, 1996.

</div>

<div id="ref-ganchev10">

\[122\] K. Ganchev, J. Graça, J. Gillenwater, and B. Taskar, “Posterior
regularization for structured latent variable models,” *Journal of
Machine Learning Research*, vol. 11, pp. 2001–2049, Mar. 2010.

</div>

<div id="ref-gannot2017consolidated">

\[123\] S. Gannot, E. Vincent, S. Markovich-Golan, and A. Ozerov, “A
consolidated perspective on multimicrophone speech enhancement and
source separation,” *IEEE/ACM Transactions on Audio, Speech, and
Language Processing*, vol. 25, no. 4, pp. 692–730, 2017.

</div>

<div id="ref-garofolo93">

\[124\] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, and D.
S. Pallett, “DARPA TIMIT acoustic-phonetic continuous speech corpus
CD-ROM. NIST speech disc 1-1.1,” *NASA STI/Recon technical report n*.
1993.

</div>

<div id="ref-gauvain94">

\[125\] J.-L. Gauvain and C.-H. Lee, “Maximum a posteriori estimation
for multivariate Gaussian mixture observations of Markov chains,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 2, no. 2,
pp. 291–298, Apr. 1994.

</div>

<div id="ref-goodfellow16">

\[126\] I. Goodfellow, Y. Bengio, and A. Courville, *Deep learning*. MIT
Press, 2016.

</div>

<div id="ref-gorlow13">

\[127\] S. Gorlow and S. Marchand, “Informed audio source separation
using linearly constrained spatial filters,” *IEEE Transactions on
Audio, Speech, and Language Processing*, vol. 21, 2013.

</div>

<div id="ref-gorlow132">

\[128\] S. Gorlow and S. Marchand, “Informed separation of spatial
images of stereo music recordings using second-order statistics,” in
*International workshop on machine learning for signal processing*,
2013.

</div>

<div id="ref-gorlow11">

\[129\] S. Gorlow and S. Marchand, “Informed source separation:
Underdetermined source signal recovery from an instantaneous stereo
mixture,” in *IEEE workshop on applications of signal processing to
audio and acoustics*, 2011.

</div>

<div id="ref-goto04">

\[130\] M. Goto, “A real-time music-scene-description system:
Predominant-F0 estimation for detecting melody and bass lines in
real-world audio signals,” *Speech Communication*, vol. 43, no. 4, pp.
311–329, Sep. 2004.

</div>

<div id="ref-grais16">

\[131\] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,
“Single channel audio source separation using deep neural network
ensembles,” in *140th aes convention*, 2016.

</div>

<div id="ref-grais162">

\[132\] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,
“Combining mask estimates for single channel audio source separation
using deep neural networks,” in *Interspeech*, 2016.

</div>

<div id="ref-grais17">

\[133\] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,
“Discriminative enhancement for single channel audio source separation
using deep neural networks,” in *13th international conference on latent
variable analysis and signal separation*, 2017.

</div>

<div id="ref-grais172">

\[134\] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,
“Two-stage single-channel audio source separation using deep neural
networks,” *IEEE/ACM Transactions on Audio, Speech, and Language
Processing*, vol. 25, no. 9, pp. 1773–1783, Sep. 2017.

</div>

<div id="ref-gregor10">

\[135\] K. Gregor and Y. LeCun, “Learning fast approximations of sparse
coding,” in *27th international conference on machine learning*, 2010.

</div>

<div id="ref-griffin03">

\[136\] D. W. Griffin and J. S. Lim, “Signal estimation from modified
short-time Fourier transform,” *IEEE Transactions on Audio, Speech, and
Language Processing*, vol. 32, no. 2, pp. 236–243, Apr. 2003.

</div>

<div id="ref-griffin88">

\[137\] D. W. Griffin and J. S. Lim, “Multiband excitation vocoder,”
*IEEE Transactions on Audio, Speech, and Language Processing*, vol. 36,
1988.

</div>

<div id="ref-grollmisch11">

\[138\] S. Grollmisch, E. Cano, and C. Dittmar, “Songs2See: Learn to
play by playing,” in *AES 41st conference: Audio for games*, 2011, pp.
P2–3.

</div>

<div id="ref-gupta15">

\[139\] U. Gupta, E. Moore, and A. Lerch, “On the perceptual relevance
of objective source separation measures for singing voice separation,”
in *IEEE workshop on applications of signal processing to audio and
acoustics*, 2005.

</div>

<div id="ref-han11">

\[140\] J. Han and C.-W. Chen, “Improving melody extraction using
probabilistic latent component analysis,” in *IEEE international
conference on acoustics, speech and signal processing*, 2011.

</div>

<div id="ref-han07">

\[141\] Y. Han and C. Raphael, “Desoloing monaural audio using mixture
models,” in *7th international conference on music information
retrieval*, 2007.

</div>

<div id="ref-hayashi16">

\[142\] A. Hayashi, H. Kameoka, T. Matsubayashi, and H. Sawada,
“Non-negative periodic component analysis for music source
separation,” in *Asia-pacific signal and information processing
association annual summit and conference*, 2016.

</div>

<div id="ref-hennequin16">

\[143\] R. Hennequin and F. Rigaud, “Long-term reverberation modeling
for under-determined audio source separation with application to vocal
melody extraction,” in *17th international society for music information
retrieval conference*, 2016.

</div>

<div id="ref-hermans13">

\[144\] M. Hermans and B. Schrauwen, “Training and analysing deep
recurrent neural networks,” in *26th international conference on neural
information processing systems*, 2013.

</div>

<div id="ref-hermansky90">

\[145\] H. Hermansky, “Perceptual linear predictive (PLP) analysis of
speech,” *Journal of the Acoustical Society of America*, vol. 87, no. 4,
pp. 1738–1752, Apr. 1990.

</div>

<div id="ref-hermes88">

\[146\] D. J. Hermes, “Measurement of pitch by subharmonic summation,”
*Journal of the Acoustical Society of America*, vol. 83, no. 1, pp.
257–264, Jan. 1988.

</div>

<div id="ref-hershey16">

\[147\] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep
clustering: Discriminative embeddings for segmentation and separation,”
in *IEEE international conference on acoustics, speech and signal
processing*, 2016.

</div>

<div id="ref-hoeting99">

\[148\] J. A. Hoeting, D. Madigan, A. E. Raftery, and C. T. Volinsky,
“Bayesian model averaging: A tutorial,” *Statistical Science*, vol.
14, no. 4, pp. 382–417, Nov. 1999.

</div>

<div id="ref-hsu09">

\[149\] C.-L. Hsu, L.-Y. Chen, J.-S. R. Jang, and H.-J. Li, “Singing
pitch extraction from monaural polyphonic songs by contextual audio
modeling and singing harmonic enhancement,” in *10th international
society for music information retrieval conference*, 2009.

</div>

<div id="ref-hsu10">

\[150\] C.-L. Hsu and J.-S. R. Jang, “On the improvement of singing
voice separation for monaural recordings using the MIR-1K dataset,”
*IEEE Transactions on Audio, Speech, and Language Processing*, vol. 18,
no. 2, pp. 310–319, Feb. 2010.

</div>

<div id="ref-hsu08">

\[151\] C.-L. Hsu, J.-S. R. Jang, and T.-L. Tsai, “Separation of singing
voice from music accompaniment with unvoiced sounds reconstruction for
monaural recordings,” in *AES 125th convention*, 2008.

</div>

<div id="ref-hsu12">

\[152\] C.-L. Hsu, D. Wang, J.-S. R. Jang, and K. Hu, “A tandem
algorithm for singing pitch extraction and voice separation from music
accompaniment,” *IEEE Transactions on Audio, Speech, and Language
Processing*, vol. 20, no. 5, pp. 1482–1491, Jul. 2012.

</div>

<div id="ref-hu10">

\[153\] G. Hu and D. Wang, “A tandem algorithm for pitch estimation and
voiced speech segregation,” *IEEE Transactions on Audio, Speech, and
Language Processing*, vol. 18, no. 8, pp. 2067–2079, Nov. 2010.

</div>

<div id="ref-hu02">

\[154\] G. Hu and D. Wang, “Monaural speech segregation based on pitch
tracking and amplitude modulation,” *IEEE Transactions on Neural
Networks*, vol. 15, no. 5, pp. 1135–1150, Sep. 2002.

</div>

<div id="ref-hartmann2004signals">

\[155\] W. M. Hartmann, *Signals, sound, and sensation*. Springer
Science & Business Media, 2004.

</div>

<div id="ref-hu16">

\[156\] Y. Hu and G. Liu, “Monaural singing voice separation by
non-negative matrix partial co-factorization with temporal continuity
and sparsity criteria,” in *12th international conference on intelligent
computing*, 2016.

</div>

<div id="ref-hu15">

\[157\] Y. Hu and G. Liu, “Separation of singing voice using nonnegative
matrix partial co-factorization for singer identification,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 23, no. 4,
pp. 643–653, Apr. 2015.

</div>

<div id="ref-huang12">

\[158\] P.-S. Huang, S. D. Chen, P. Smaragdis, and M. Hasegawa-Johnson,
“Singing-voice separation from monaural recordings using robust
principal component analysis,” in *IEEE international conference on
acoustics, speech and signal processing*, 2012.

</div>

<div id="ref-huang15">

\[159\] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,
“Joint optimization of masks and deep recurrent neural networks for
monaural source separation,” *IEEE/ACM Transactions on Audio, Speech,
and Language Processing*, vol. 23, 2015.

</div>

<div id="ref-huang14">

\[160\] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,
“Singing-voice separation from monaural recordings using deep
recurrent neural networks,” in *15th international society for music
information retrieval conference*, 2014.

</div>

<div id="ref-huang142">

\[161\] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,
“Deep learning for monaural speech separation,” in *IEEE international
conference on acoustics, speech and signal processing*, 2014.

</div>

<div id="ref-hyvarinen99">

\[162\] A. Hyvärinen, “Fast and robust fixed-point algorithm for
independent component analysis,” *IEEE Transactions on Neural Networks*,
vol. 10, no. 3, pp. 626–634, May 1999.

</div>

<div id="ref-hyvarinen00">

\[163\] A. Hyvärinen and E. Oja, “Independent component analysis:
Algorithms and applications,” *Neural Networks*, vol. 13, nos. 4-5, pp.
411–430, Jun. 2000.

</div>

<div id="ref-ikemiya16">

\[164\] Y. Ikemiya, K. Itoyama, and K. Yoshii, “Singing voice separation
and vocal F0 estimation based on mutual combination of robust principal
component analysis and subharmonic summation,” *IEEE/ACM Transactions on
Audio, Speech, and Language Processing*, vol. 24, no. 11, pp. 2084–2095,
Nov. 2016.

</div>

<div id="ref-ikemiya15">

\[165\] Y. Ikemiya, K. Yoshii, and K. Itoyama, “Singing voice analysis
and editing based on mutually dependent F0 estimation and source
separation,” in *IEEE international conference on acoustics, speech and
signal processing*, 2015.

</div>

<div id="ref-isik16">

\[166\] Y. Isik, J. L. Roux, Z. Chen, S. Watanabe, and J. R. Hershey,
“Single-channel multispeaker separation using deep clustering,” in
*Interspeech*, 2016.

</div>

<div id="ref-itakura68">

\[167\] F. Itakura and S. Saito, “Analysis synthesis telephony based on
the maximum likelihood method,” in *International congress on
acoustics*, 1968.

</div>

<div id="ref-jain90">

\[168\] A. K. Jain and F. Farrokhnia, “Unsupervised texture segmentation
using Gabor filters,” in *IEEE international conference on systems, man
and cybernetics*, 1990.

</div>

<div id="ref-janer13">

\[169\] J. Janer and R. Marxer, “Separation of unvoiced fricatives in
singing voice mixtures with semi-supervised NMF,” in *16th international
conference on digital audio effects*, 2013.

</div>

<div id="ref-janer12">

\[170\] J. Janer, R. Marxer, and K. Arimoto, “Combining a harmonic-based
NMF decomposition with transient analysis for instantaneous percussion
separation,” in *IEEE international conference on acoustics, speech and
signal processing*, 2012.

</div>

<div id="ref-jansson17">

\[171\] A. Jansson, E. Humphrey, N. Montecchio, R. Bittner, A. Kumar,
and T. Weyde, “Singing voice separation with deep U-Net convolutional
networks,” in *18th international society for music information
retrieval conferenceng*, 2017.

</div>

<div id="ref-jaureguiberry13">

\[172\] X. Jaureguiberry, G. Richard, P. Leveau, R. Hennequin, and E.
Vincent, “Introducing a simple fusion framework for audio source
separation,” in *IEEE international workshop on machine learning for
signal processing*, 2013.

</div>

<div id="ref-jaureguiberry16">

\[173\] X. Jaureguiberry, E. Vincent, and G. Richard, “Fusion methods
for speech enhancement and audio source separation,” *IEEE/ACM
Transactions on Audio, Speech, and Language Processing*, vol. 24, no. 7,
pp. 1266–1279, Jul. 2016.

</div>

<div id="ref-jaureguiberry14">

\[174\] X. Jaureguiberry, E. Vincent, and G. Richard, “Variational
Bayesian model averaging for audio source separation,” in *IEEE workshop
on statistical signal processing workshop*, 2014.

</div>

<div id="ref-jaynes2003probability">

\[175\] E. T. Jaynes, *Probability theory: The logic of science*.
Cambridge university press, 2003.

</div>

<div id="ref-jeong17">

\[176\] I.-Y. Jeong and K. Lee, “Singing voice separation using RPCA
with weighted \(l_1\)-norm,” in *13th international conference on latent
variable analysis and signal separation*, 2017.

</div>

<div id="ref-jeong14">

\[177\] I.-Y. Jeong and K. Lee, “Vocal separation using extended robust
principal component analysis with Schatten \(P\)/\(L_p\)-norm and scale
compression,” in *International workshop on machine learning for signal
processing*, 2014.

</div>

<div id="ref-jeong142">

\[178\] I.-Y. Jeong and K. Lee, “Vocal separation from monaural music
using temporal/spectral continuity and sparsity constraints,” *IEEE
Signal Processing Letters*, vol. 21, no. 10, pp. 1197–1200, Jun. 2014.

</div>

<div id="ref-joder11">

\[179\] C. Joder, S. Essid, and G. Richard, “A conditional random field
framework for robust and scalable audio-to-score matching,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 19, no. 8,
pp. 2385–2397, Nov. 2011.

</div>

<div id="ref-joder12">

\[180\] C. Joder and B. Schuller, “Score-informed leading voice
separation from monaural audio,” in *13th international society for
music information retrieval conference*, 2012.

</div>

<div id="ref-jourjine00">

\[181\] A. Jourjine, S. Rickard, and Ö. Yilmaz, “Blind separation of
disjoint orthogonal signals: Demixing n sources from 2 mixtures,” in
*IEEE international conference on acoustics, speech and signal
processing*, 2000.

</div>

<div id="ref-kalakota00">

\[182\] R. Kalakota and M. Robinson, *E-business 2.0: Roadmap for
success*. Addison-Wesley Professional, 2000.

</div>

<div id="ref-kameoka06">

\[183\] H. Kameoka, M. Goto, and S. Sagayama, “Selective amplifier of
periodic and non-periodic components in concurrent audio signals with
spectral control envelopes,” Information Processing Society of Japan,
2006.

</div>

<div id="ref-khurshid04">

\[184\] A. Khurshid and S. L. Denham, “A temporal-analysis-based pitch
estimation system for noisy speech with a comparative study of
performance of recent systems,” *IEEE Transactions on Neural Networks*,
vol. 15, no. 5, pp. 1112–1124, Sep. 2004.

</div>

<div id="ref-kim16">

\[185\] H.-G. Kim and J. Y. Kim, “Music/voice separation based on kernel
back-fitting using weighted \(\beta\)-order MMSE estimation,” *ETRI
Journal*, vol. 38, no. 3, pp. 510–517, Jun. 2016.

</div>

<div id="ref-kim11">

\[186\] M. Kim, S. Beack, K. Choi, and K. Kang, “Gaussian mixture model
for singing voice separation from stereophonic music,” in *AES 43rd
conference*, 2011.

</div>

<div id="ref-kim152">

\[187\] M. Kim and P. Smaragdis, “Mixtures of local dictionaries for
unsupervised speech enhancement,” *IEEE Signal Processing Letters*, vol.
22, 2016.

</div>

<div id="ref-kim15">

\[188\] M. Kim and P. Smaragdis, “Adaptive denoising autoencoders: A
fine-tuning scheme to learn from test mixtures,” in *12th international
conference on latent variable analysis and signal separation*, 2015.

</div>

<div id="ref-kim112">

\[189\] M. Kim, J. Yoo, K. Kang, and S. Choi, “Nonnegative matrix
partial co-factorization for spectral and temporal drum source
separation,” *IEEE Journal of Selected Topics in Signal Processing*,
vol. 5, no. 6, pp. 1192–1204, Oct. 2011.

</div>

<div id="ref-klapuri06">

\[190\] A. Klapuri, “Multiple fundamental frequency estimation by
summing harmonic amplitudes,” in *7th international conference on music
information retrieval*, 2006.

</div>

<div id="ref-klatt90">

\[191\] D. H. Klatt and L. C. Klatt, “Analysis, synthesis, and
perception of voice quality variations among female and male talkers,”
*Journal of the Acoustical Society of America*, vol. 87, no. 2, pp.
820–857, Feb. 1990.

</div>

<div id="ref-kornycky08">

\[192\] J. Kornycky, B. Gunel, and A. Kondoz, “Comparison of subjective
and objective evaluation methods for audio source separation,” *Journal
of the Acoustical Society of America*, vol. 4, no. 1, 2008.

</div>

<div id="ref-lacoste-julien13">

\[193\] S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher,
“Block-coordinate Frank-Wolfe optimization for structural SVMs,” in
*30th international conference on machine learning*, 2013.

</div>

<div id="ref-lagrange08">

\[194\] M. Lagrange, L. G. Martins, J. Murdoch, and G. Tzanetakis,
“Normalized cuts for predominant melodic source separation,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 16, no. 2,
pp. 278–290, Feb. 2008.

</div>

<div id="ref-lagrange07">

\[195\] M. Lagrange and G. Tzanetakis, “Sound source tracking and
formation using normalized cuts,” in *IEEE international conference on
acoustics, speech and signal processing*, 2007.

</div>

<div id="ref-lam01">

\[196\] C. K. Lam and B. C. Tan, “The Internet is changing the music
industry,” *Communications of the ACM*, vol. 44, no. 8, pp. 62–68, 2001.

</div>

<div id="ref-laurent16">

\[197\] C. Laurent, G. Pereyra, P. Brakel, Y. Zhang, and Y. Bengio,
“Batch normalized recurrent neural networks,” in *IEEE international
conference on acoustics, speech and signal processing*, 2016.

</div>

<div id="ref-lawton71">

\[198\] W. H. Lawton and E. A. Sylvestre, “Self modeling curve
resolution,” *Technometrics*, vol. 13, 1971.

</div>

<div id="ref-lecun15">

\[199\] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*,
vol. 521, pp. 436–444, May 2015.

</div>

<div id="ref-lee01">

\[200\] D. D. Lee and H. S. Seung, “Algorithms for non-negative matrix
factorization,” in *Advances in neural information processing systems
13*, MIT Press, 2001, pp. 556–562.

</div>

<div id="ref-lee99">

\[201\] D. D. Lee and H. S. Seung, “Learning the parts of objects by
non-negative matrix factorization,” *Nature*, vol. 401, pp. 788–791,
Oct. 1999.

</div>

<div id="ref-lee15">

\[202\] J.-Y. Lee, H.-S. Cho, and H.-G. Kim, “Vocal separation from
monaural music using adaptive auditory filtering based on kernel
back-fitting,” in *Interspeech*, 2015.

</div>

<div id="ref-lee152">

\[203\] J.-Y. Lee and H.-G. Kim, “Music and voice separation using
log-spectral amplitude estimator based on kernel spectrogram models
backfitting,” *Journal of the Acoustical Society of Korea*, vol. 34, no.
3, pp. 227–233, 2015.

</div>

<div id="ref-lefevre13">

\[204\] A. Lefèvre, F. Glineur, and P.-A. Absil, “A nuclear-norm based
convex formulation for informed source separation,” in *21st european
symposium on artificial neural networks, computational intelligence and
machine learning*, 2013.

</div>

<div id="ref-leglaive15">

\[205\] S. Leglaive, R. Hennequin, and R. Badeau, “Singing voice
detection with deep recurrent neural networks,” in *IEEE international
conference on acoustics, speech and signal processing*, 2015.

</div>

<div id="ref-lehner14">

\[206\] B. Lehner, G. Widmer, and R. Sonnleitner, “On the reduction of
false positives in singing voice detection,” in *IEEE international
conference on acoustics, speech and signal processing*, 2014.

</div>

<div id="ref-leroux15">

\[207\] J. L. Roux, J. R. Hershey, and F. Weninger, “Deep NMF for speech
separation,” in *IEEE international conference on acoustics, speech and
signal processing*, 2015.

</div>

<div id="ref-li07">

\[208\] Y. Li and D. Wang, “Separation of singing voice from music
accompaniment for monaural recordings,” *IEEE Transactions on Audio,
Speech, and Language Processing*, vol. 15, no. 4, pp. 1475–1487, May
2007.

</div>

<div id="ref-li06">

\[209\] Y. Li and D. Wang, “Singing voice separation from monaural
recordings,” in *7th international conference on music information
retrieval*, 2006.

</div>

<div id="ref-li05">

\[210\] Y. Li and D. Wang, “Detecting pitch of singing voice in
polyphonic audio,” in *IEEE international conference on acoustics,
speech and signal processing*, 2005.

</div>

<div id="ref-li09">

\[211\] Y. Li, J. Woodruff, and D. Wang, “Monaural musical sound
separation based on pitch and common amplitude modulation,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 17, no. 7,
pp. 1361–1371, Sep. 2009.

</div>

<div id="ref-lin09">

\[212\] Z. Lin, M. Chen, L. Wu, and Y. Ma, “The augmented Lagrange
multiplier method for exact recovery of corrupted low-rank matrices,”
University of Illinois at Urbana-Champaign, 2009.

</div>

<div id="ref-liu13">

\[213\] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, “Robust
recovery of subspace structures by low-rank representation,” *IEEE
Transactions on Pattern Analysis and Machine Intelligence*, vol. 35, no.
1, pp. 171–184, Jan. 2007.

</div>

<div id="ref-liutkus11">

\[214\] A. Liutkus, R. Badeau, and G. Richard, “Gaussian processes for
underdetermined source separation,” *IEEE Transactions on Audio, Speech,
and Language Processing*, vol. 59, no. 7, pp. 3155–3167, Feb. 2011.

</div>

<div id="ref-liutkus10">

\[215\] A. Liutkus, R. Badeau, and G. Richard, “Informed source
separation using latent components,” in *9th international conference on
latent variable analysis and signal separation*, 2010.

</div>

<div id="ref-liutkus12">

\[216\] A. Liutkus, Z. Rafii, R. Badeau, B. Pardo, and G. Richard,
“Adaptive filtering for music/voice separation exploiting the
repeating musical structure,” in *IEEE international conference on
acoustics, speech and signal processing*, 2012.

</div>

<div id="ref-liutkus122">

\[217\] A. Liutkus, J. Pinel, R. Badeau, L. Girin, and G. Richard,
“Informed source separation through spectrogram coding and data
embedding,” *Signal Processing*, vol. 92, 2012.

</div>

<div id="ref-liutkus13">

\[218\] A. Liutkus, J.-L. Durrieu, L. Daudet, and G. Richard, “An
overview of informed audio source separation,” in *14th international
workshop on image analysis for multimedia interactive services*, 2013.

</div>

<div id="ref-liutkus14">

\[219\] A. Liutkus, Z. Rafii, B. Pardo, D. FitzGerald, and L. Daudet,
“Kernel spectrogram models for source separation,” in *4th joint
workshop on hands-free speech communication microphone arrays*, 2014.

</div>

<div id="ref-liutkus142">

\[220\] A. Liutkus, D. FitzGerald, Z. Rafii, B. Pardo, and L. Daudet,
“Kernel additive models for source separation,” *IEEE Transactions on
Signal Processing*, vol. 62, no. 16, pp. 4298–4310, Aug. 2014.

</div>

<div id="ref-liutkus15">

\[221\] A. Liutkus, D. FitzGerald, and Z. Rafii, “Scalable audio
separation with light kernel additive modelling,” in *IEEE international
conference on acoustics, speech and signal processing*, 2015.

</div>

<div id="ref-liutkus15b">

\[222\] A. Liutkus, D. Fitzgerald, and R. Badeau, “Cauchy nonnegative
matrix factorization,” in *IEEE workshop on applications of signal
processing to audio and acoustics*, 2015.

</div>

<div id="ref-liutkus15c">

\[223\] A. Liutkus and R. Badeau, “Generalized Wiener filtering with
fractional power spectrograms,” in *IEEE international conference on
acoustics, speech and signal processing*, 2015.

</div>

<div id="ref-liutkus17">

\[224\] A. Liutkus *et al.*, “The 2016 signal separation evaluation
campaign,” in *13th international conference on latent variable analysis
and signal separation*, 2017.

</div>

<div id="ref-loizou13">

\[225\] P. C. Loizou, *Speech enhancement: Theory and practice*. CRC
Press, 1990.

</div>

<div id="ref-luo17">

\[226\] Y. Luo, Z. Chen, J. R. Hershey, J. L. Roux, and N. Mesgarani,
“Deep clustering and conventional networks for music separation:
Stronger together,” in *IEEE international conference on acoustics,
speech and signal processing*, 2017.

</div>

<div id="ref-muller2015">

\[227\] M. Müller, *Fundamentals of music processing: Audio, analysis,
algorithms, applications*. Springer, 2015.

</div>

<div id="ref-ma16">

\[228\] S. Ma, “Alternating proximal gradient method for convex
minimization,” *Journal of Scientific Computing*, vol. 68, no. 2, pp.
546–572, Aug. 2016.

</div>

<div id="ref-maher89">

\[229\] R. C. Maher, “An approach for the separation of voices in
composite musical signals,” PhD thesis, University of Illinois at
Urbana-Champaign, 1989.

</div>

<div id="ref-mairal09">

\[230\] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online dictionary
learning for sparse coding,” in *26th annual international conference on
machine learning*, 2009.

</div>

<div id="ref-makino07">

\[231\] S. Makino, T.-W. Lee, and H. Sawada, *Blind speech separation*.
Springer Netherlands, 2007.

</div>

<div id="ref-mallat93">

\[232\] S. G. Mallat and Z. Zhang, “Matching pursuits with
time-frequency dictionaries,” *IEEE Transactions on Signal Processing*,
vol. 41, no. 12, pp. 3397–3415, Dec. 1993.

</div>

<div id="ref-manilow17">

\[233\] E. Manilow, P. Seetharaman, F. Pishdadian, and B. Pardo,
“Predicting algorithm efficacy for adaptive, multi-cue source
separation,” in *IEEE workshop on applications of signal processing to
audio and acoustics*, 2017.

</div>

<div id="ref-margulis14">

\[234\] E. H. Margulis, *On repeat: How music plays the mind*. Oxford
University Press, 2014.

</div>

<div id="ref-marxer13">

\[235\] R. Marxer and J. Janer, “Modelling and separation of singing
voice breathiness in polyphonic mixtures,” in *16th international
conference on digital audio effects*, 2013.

</div>

<div id="ref-marxer132">

\[236\] R. Marxer and J. Janer, “Low-latency bass separation using
harmonic-percussion decomposition,” in *16th international conference on
digital audio effects*, 2013.

</div>

<div id="ref-marxer122">

\[237\] R. Marxer and J. Janer, “A Tikhonov regularization method for
spectrum decomposition in low latency audio source separation,” in *IEEE
international conference on acoustics, speech and signal processing*,
2012.

</div>

<div id="ref-marxer12">

\[238\] R. Marxer, J. Janer, and J. Bonada, “Low-latency instrument
separation in polyphonic audio using timbre models,” in *10th
international conference on latent variable analysis and signal
separation*, 2012.

</div>

<div id="ref-matz15">

\[239\] D. Matz, E. Cano, and J. Abeßer, “New sonorities for early jazz
recordings using sound source separation and automatic mixing tools,” in
*16th international society on music information retrieval conference*,
2015.

</div>

<div id="ref-mcaulay86">

\[240\] R. J. McAulay and T. F. Quatieri, “Speech analysis/synthesis
based on a sinusoidal representation,” *IEEE Transactions on Audio,
Speech, and Language Processing*, vol. 34, no. 4, pp. 744–754, Aug.
1986.

</div>

<div id="ref-mcdermott11">

\[241\] J. McDermott, D. Wrobleski, and A. J. Oxenham, “Recovering sound
sources from embedded repetition,” *Proceedings of the National Academy
of Sciences*, vol. 108, 2011.

</div>

<div id="ref-mcvicar16">

\[242\] M. McVicar, R. Santos-Rodriguez, and T. D. Bie, “Learning to
separate vocals from polyphonic mixtures via ensemble methods and
structured output prediction,” in *IEEE international conference on
acoustics, speech and signal processing*, 2016.

</div>

<div id="ref-meron98">

\[243\] Y. Meron and K. Hirose, “Separation of singing and piano
sounds,” in *5th international conference on spoken language
processing*, 1998.

</div>

<div id="ref-mesaros07">

\[244\] A. Mesaros, T. Virtanen, and A. Klapuri, “Singer identification
in polyphonic music using vocal separation and pattern recognition
methods,” in *7th international conference on music information
retrieval*, 2007.

</div>

<div id="ref-miller73">

\[245\] N. J. Miller, “Removal of noise from a voice signal by
synthesis,” Utah University, 1973.

</div>

<div id="ref-mimilakis16">

\[246\] S. I. Mimilakis, E. Cano, J. Abeßer, and G. Schuller, “New
sonorities for jazz recordings: Separation and mixing using deep neural
networks,” in *2nd aes workshop on intelligent music production*, 2016.

</div>

<div id="ref-mimilakis162">

\[247\] S. I. Mimilakis, K. Drossos, T. Virtanen, and G. Schuller, “Deep
neural networks for dynamic range compression in mastering
applications,” in *140th aes convention*, 2016.

</div>

<div id="ref-mimilakis17">

\[248\] S. I. Mimilakis, K. Drossos, T. Virtanen, and G. Schuller, “A
recurrent encoder-decoder approach with skip-filtering connections for
monaural singing voice separation,” in *IEEE international workshop on
machine learning for signal processing*, 2017.

</div>

<div id="ref-mimilakis172">

\[249\] S. I. Mimilakis, K. Drossos, J. F. Santos, G. Schuller, T.
Virtanen, and Y. Bengio, “Monaural singing voice separation with
skip-filtering connections and recurrent inference of time-frequency
mask,” in *IEEE international conference on acoustics, speech and signal
processing*, 2018.

</div>

<div id="ref-mohammadiha13">

\[250\] N. Mohammadiha, P. Smaragdis, and A. Leijon, “Prediction based
filtering and smoothing to exploit temporal dependencies in NMF,” in
*IEEE international conference on acoustics, speech and signal
processing*, 2013.

</div>

<div id="ref-moorer05">

\[251\] J. A. Moorer, “Signal processing aspects of computer music: A
survey,” *Proceedings of the IEEE*, vol. 65, no. 8, pp. 1108–1137, Aug.
2005.

</div>

<div id="ref-moussallam12">

\[252\] M. Moussallam, G. Richard, and L. Daudet, “Audio source
separation informed by redundancy with greedy multiscale
decompositions,” in *20th european signal processing conference*,
2012.

</div>

<div id="ref-mysore10">

\[253\] G. J. Mysore, P. Smaragdis, and B. Raj, “Non-negative hidden
Markov modeling of audio with application to source separation,” in *9th
international conference on latent variable analysis and signal
separation*, 2010.

</div>

<div id="ref-naik15">

\[254\] G. R. Naik, *Non-negative matrix factorization techniques*.
Springer, 2015.

</div>

<div id="ref-naik14">

\[255\] G. R. Naik and W. Wang, *Blind source separation*.
Springer-Verlag Berlin Heidelberg, 2014.

</div>

<div id="ref-nakamuray15">

\[256\] T. Nakamuray and H. Kameoka, “\(L_p\)-norm non-negative matrix
factorization and its application to singing voice enhancement,” in
*IEEE international conference on acoustics, speech and signal
processing*, 2015.

</div>

<div id="ref-nie152">

\[257\] F. Nie, H. Wang, and H. Huang, “Joint Schatten \(p\)-norm and
\(l_p\)-norm robust matrix completion for missing value recovery,”
*Knowledge and Information Systems*, vol. 42, no. 3, pp. 525–544, Mar.
2015.

</div>

<div id="ref-nie15">

\[258\] S. Nie *et al.*, “Joint optimization of recurrent networks
exploiting source auto-regression for source separation,” in
*Interspeech*, 2015.

</div>

<div id="ref-noh15">

\[259\] H. Noh, S. Hong, and B. Han, “Learning deconvolution network for
semantic segmentation,” in *IEEE international conference on computer
vision*, 2015.

</div>

<div id="ref-noll67">

\[260\] A. M. Noll, “Cepstrum pitch determination,” *Journal of the
Acoustical Society of America*, vol. 41, no. 2, pp. 293–309, 1967.

</div>

<div id="ref-noll64">

\[261\] A. M. Noll, “Short-time spectrum and ‘cepstrum’ techniques for
vocal-pitch detection,” *Journal of the Acoustical Society of America*,
vol. 36, no. 2, pp. 296–302, 1964.

</div>

<div id="ref-nolte12">

\[262\] G. Nolte *et al.*, “The 2011 signal separation evaluation
campaign (SiSEC2011): - biomedical data analysis -,” in *10th
international conference on latent variable analysis and signal
separation*, 2012.

</div>

<div id="ref-nugraha16">

\[263\] A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichannel music
separation with deep neural networks,” in *24th european signal
processing conference*, 2016.

</div>

<div id="ref-nugraha162">

\[264\] A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichannel audio
source separation with deep neural networks,” *IEEE/ACM Transactions on
Audio, Speech, and Language Processing*, vol. 24, no. 9, pp. 1652–1664,
Sep. 2016.

</div>

<div id="ref-nugraha15">

\[265\] A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichannel audio
source separation with deep neural networks,” Inria, 2015.

</div>

<div id="ref-nwe04">

\[266\] T. L. Nwe and Y. Wang, “Automatic detection of vocal segments in
popular songs,” in *5th international conference for music information
retrieval*, 2004.

</div>

<div id="ref-ogrady05">

\[267\] P. D. O’Grady, B. A. Pearlmutter, and S. T. Rickard, “Survey of
sparse and non-sparse methods in source separation,” *International
Journal of Imaging Systems and Technology*, vol. 15, 2005.

</div>

<div id="ref-ochiai15">

\[268\] E. Ochiai, T. Fujisawa, and M. Ikehara, “Vocal separation by
constrained non-negative matrix factorization,” in *Asia-pacific signal
and information processing association annual summit and conference*,
2015.

</div>

<div id="ref-ono10">

\[269\] N. Ono *et al.*, “Harmonic and percussive sound separation and
its application to MIR-related tasks,” in *Advances in music information
retrieval*, Springer Berlin Heidelberg, 2010, pp. 213–236.

</div>

<div id="ref-ono082">

\[270\] N. Ono, K. Miyamoto, H. Kameoka, and S. Sagayama, “A real-time
equalizer of harmonic and percussive components in music signals,” in
*9th international conference on music information retrieval*, 2008.

</div>

<div id="ref-ono08">

\[271\] N. Ono, K. Miyamoto, J. L. Roux, H. Kameoka, and S. Sagayama,
“Separation of a monaural audio signal into harmonic/percussive
components by complementary diffusion on spectrogram,” in *16th european
signal processing conference*, 2008.

</div>

<div id="ref-ono15">

\[272\] N. Ono, Z. Rafii, D. Kitamura, N. Ito, and A. Liutkus, “The 2015
signal separation evaluation campaign,” in *12th international
conference on latent variable analysis and signal separation*, 2015.

</div>

<div id="ref-oppenheim69">

\[273\] A. V. Oppenheim, “Speech analysis-synthesis system based on
homomorphic filtering,” *Journal of the Acoustical Society of America*,
vol. 45, no. 2, pp. 458–465, 1969.

</div>

<div id="ref-oppenheim68">

\[274\] A. V. Oppenheim and R. W. Schafer, “Homomorphic analysis of
speech,” *IEEE Transactions on Audio and Electroacoustics*, vol. 16, no.
2, pp. 221–226, Jun. 1968.

</div>

<div id="ref-oppenheim682">

\[275\] A. V. Oppenheim, R. W. Schafer, and T. G. Stockham, “Nonlinear
filtering of multiplied and convolved signals,” *IEEE Transactions on
Audio and Electroacoustics*, vol. 16, 1968.

</div>

<div id="ref-ortega70">

\[276\] J. M. Ortega and W. C. Rheinboldt, *Iterative solution of
nonlinear equations in several variables*. Academic Press, 1970.

</div>

<div id="ref-otsu79">

\[277\] N. Otsu, “A threshold selection method from gray-level
histograms,” *IEEE Transactions on Systems, Man, and Cybernetics*, vol.
9, no. 1, pp. 62–66, Jan. 1979.

</div>

<div id="ref-ozerov14">

\[278\] A. Ozerov, N. Duong, and L. Chevallier, “On monotonicity of
multiplicative update rules for weighted nonnegative tensor
factorization,” in *International symposium on nonlinear theory and its
applications*, 2014.

</div>

<div id="ref-ozerov13">

\[279\] A. Ozerov, N. Duong, and L. Chevallier, “Weighted nonnegative
tensor factorization: On monotonicity of multiplicative update rules and
application to user-guided audio source separation,” Technicolor, 2013.

</div>

<div id="ref-ozerov10">

\[280\] A. Ozerov and C. Févotte, “Multichannel nonnegative matrix
factorization in convolutive mixtures for audio source separation,”
*IEEE Transactions on Audio, Speech, and Language Processing*, vol. 18,
no. 3, pp. 550–563, Mar. 2010.

</div>

<div id="ref-ozerov09">

\[281\] A. Ozerov and C. Févotte, “Multichannel nonnegative matrix
factorization in convolutive mixtures with application to blind audio
source separation,” in *IEEE international conference on acoustics,
speech and signal processing*, 2009.

</div>

<div id="ref-ozerov11">

\[282\] A. Ozerov, C. Févotte, R. Blouet, and J.-L. Durrieu,
“Multichannel nonnegative tensor factorization with structured
constraints for user-guided audio source separation,” in *IEEE
international conference on acoustics, speech and signal processing*,
2011.

</div>

<div id="ref-ozerov112">

\[283\] A. Ozerov, A. Liutkus, R. Badeau, and G. Richard, “Informed
source separation: Source coding meets source separation,” in *IEEE
workshop on applications of signal processing to audio and acoustics*,
2011.

</div>

<div id="ref-ozerov07">

\[284\] A. Ozerov, P. Philippe, F. Bimbot, and R. Gribonval, “Adaptation
of Bayesian models for single-channel source separation and its
application to voice/music separation in popular songs,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 15, no. 5,
pp. 1564–1578, Jul. 2007.

</div>

<div id="ref-ozerov05">

\[285\] A. Ozerov, P. Philippe, R. Gribonval, and F. Bimbot, “One
microphone singing voice separation using source-adapted models,” in
*IEEE workshop on applications of signal processing to audio and
acoustics*, 2005.

</div>

<div id="ref-ozerov12">

\[286\] A. Ozerov, E. Vincent, and F. Bimbot, “A general flexible
framework for the handling of prior information in audio source
separation,” *IEEE Transactions on Audio, Speech, and Language
Processing*, vol. 20, no. 4, pp. 1118–1133, May 2012.

</div>

<div id="ref-ozerov102">

\[287\] A. Ozerov, E. Vincent, and F. Bimbot, “A general modular
framework for audio source separation,” in *9th international conference
on latent variable analysis and signal separation*, 2010.

</div>

<div id="ref-paatero97">

\[288\] P. Paatero, “Least squares formulation of robust non-negative
factor analysis,” *Chemometrics and Intelligent Laboratory Systems*,
vol. 37, 1997.

</div>

<div id="ref-paatero94">

\[289\] P. Paatero and U. Tapper, “Positive matrix factorization: A
non-negative factor model with optimal utilization of error estimates of
data values,” *Environmetrics*, vol. 5, 1994.

</div>

<div id="ref-papadopoulos14">

\[290\] H. Papadopoulos and D. P. Ellis, “Music-content-adaptive robust
principal component analysis for a semantically consistent separation of
foreground and background in music audio signals,” in *17th
international conference on digital audio effects*, 2014.

</div>

<div id="ref-parvaix10">

\[291\] M. Parvaix and L. Girin, “Informed source separation of linear
instantaneous under-determined audio mixtures by source index
embedding,” *IEEE Transactions on Audio, Speech, and Language
Processing*, vol. 19, 2010.

</div>

<div id="ref-parvaix103">

\[292\] M. Parvaix and L. Girin, “Informed source separation of
underdetermined instantaneous stereo mixtures using source index
embedding,” in *IEEE international conference on acoustics, speech and
signal processing*, 2010.

</div>

<div id="ref-parvaix102">

\[293\] M. Parvaix, L. Girin, and J.-M. Brossier, “A watermarking-based
method for informed source separation of audio signals with a single
sensor,” *IEEE Transactions on Audio, Speech, and Language Processing*,
vol. 18, 2010.

</div>

<div id="ref-parvaix09">

\[294\] M. Parvaix, L. Girin, and J.-M. Brossier, “A watermarking-based
method for single-channel audio source separation,” in *IEEE
international conference on acoustics, speech and signal processing*,
2009.

</div>

<div id="ref-parvaix104">

\[295\] M. Parvaix, L. Girin, L. Daudet, J. Pinel, and C. Baras, “Hybrid
coding/indexing strategy for informed source separation of linear
instantaneous under-determined audio mixtures,” in *International
congress on acoustics*, 2010.

</div>

<div id="ref-pascanu14">

\[296\] R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio, “How to
construct deep recurrent neural networks,” in *International conference
on learning representations*, 2014.

</div>

<div id="ref-patterson87">

\[297\] R. D. Patterson, I. Nimmo-Smith, J. Holdsworth, and P. Rice, “An
efficient auditory filterbank based on the gammatone function,” in *A
meeting of the ioc speech group on auditory modelling at rsre*, 1987.

</div>

<div id="ref-paulus10">

\[298\] J. Paulus, M. Müller, and A. Klapuri, “Audio-based music
structure analysis,” in *11th international society for music
information retrieval conference*, 2010.

</div>

<div id="ref-peeters03">

\[299\] G. Peeters, “Deriving musical structures from signal analysis
for music audio summary generation: "Sequence" and "state" approach,” in
*International symposium on computer music multidisciplinary research*,
2003.

</div>

<div id="ref-pham14">

\[300\] V. Pham, T. Bluche, C. Kermorvant, and J. Louradour, “Dropout
improves recurrent neural networks for handwriting recognition,” in
*International conference on frontiers in handwriting recognition*,
2014.

</div>

<div id="ref-piccardi04">

\[301\] M. Piccardi, “Background subtraction techniques: A review,” in
*International conference on systems, man and cybernetics*, 2004.

</div>

<div id="ref-pinel14">

\[302\] J. Pinel, L. Girin, and C. Baras, “A high-rate data hiding
technique for uncompressed audio signals,” *Journal of the Audio
Engineering Society*, vol. 62, 2014.

</div>

<div id="ref-pinel10">

\[303\] J. Pinel, L. Girin, C. Baras, and M. Parvaix, “A high-capacity
watermarking technique for audio signals based on MDCT-domain
quantization,” in *International congress on acoustics*, 2010.

</div>

<div id="ref-plourde08">

\[304\] E. Plourde and B. Champagne, “Auditory-based spectral amplitude
estimators for speech enhancement,” *IEEE Transactions on Audio, Speech,
and Language Processing*, vol. 16, no. 8, pp. 1614–1623, Nov. 2008.

</div>

<div id="ref-pratzlich15">

\[305\] T. Prätzlich, R. Bittner, A. Liutkus, and M. Müller, “Kernel
additive modeling for interference reduction in multi-channel music
recordings,” in *IEEE international conference on acoustics, speech and
signal processing*, 2015.

</div>

<div id="ref-qian17">

\[306\] K. Qian, Y. Zhang, S. Chang, X. Yang, D. Florêncio, and M.
Hasegawa-Johnson, “Speech enhancement using bayesian wavenet,” *Proc.
Interspeech 2017*, pp. 2013–2017, 2017.

</div>

<div id="ref-quatieri92">

\[307\] T. F. Quatieri, “Shape invariant time-scale and pitch
modification of speech,” *IEEE Transactions on Signal Processing*, vol.
40, no. 3, pp. 497–510, Mar. 1992.

</div>

<div id="ref-rabiner89">

\[308\] L. R. Rabiner, “A tutorial on hidden Markov models and selected
applications in speech recognition,” *Proceedings of the IEEE*, vol. 77,
no. 2, pp. 257–286, Feb. 1989.

</div>

<div id="ref-raffel14">

\[309\] C. Raffel *et al.*, “Mir\_eval: A transparent implementation of
common MIR metrics,” in *15th international society for music
information retrieval conference*, 2014.

</div>

<div id="ref-rafii142">

\[310\] Z. Rafii, Z. Duan, and B. Pardo, “Combining rhythm-based and
pitch-based methods for background and melody separation,” *IEEE/ACM
Transactions on Audio, Speech, and Language Processing*, vol. 22, no.
12, pp. 1884–1893, Sep. 2014.

</div>

<div id="ref-rafii15">

\[311\] Z. Rafii, A. Liutkus, and B. Pardo, “A simple user interface
system for recovering patterns repeating in time and frequency in
mixtures of sounds,” in *IEEE international conference on acoustics,
speech and signal processing*, 2015.

</div>

<div id="ref-rafii14">

\[312\] Z. Rafii, A. Liutkus, and B. Pardo, “REPET for
background/foreground separation in audio,” in *Blind source
separation*, Springer Berlin Heidelberg, 2014, pp. 395–411.

</div>

<div id="ref-rafii13">

\[313\] Z. Rafii and B. Pardo, “REpeating Pattern Extraction Technique
(REPET): A simple method for music/voice separation,” *IEEE Transactions
on Audio, Speech, and Language Processing*, vol. 21, no. 1, pp. 73–84,
Jan. 2013.

</div>

<div id="ref-rafii133">

\[314\] Z. Rafii and B. Pardo, “Online REPET-SIM for real-time speech
enhancement,” in *IEEE international conference on acoustics, speech and
signal processing*, 2013.

</div>

<div id="ref-rafii12">

\[315\] Z. Rafii and B. Pardo, “Music/voice separation using the
similarity matrix,” in *13th international society for music information
retrieval conference*, 2012.

</div>

<div id="ref-rafii11">

\[316\] Z. Rafii and B. Pardo, “A simple music/voice separation system
based on the extraction of the repeating musical structure,” in *IEEE
international conference on acoustics, speech and signal processing*,
2011.

</div>

<div id="ref-rafii132">

\[317\] Z. Rafii, D. L. Sun, F. G. Germain, and G. J. Mysore, “Combining
modeling of singing voice and background music for automatic separation
of musical mixtures,” in *14th international society for music
information retrieval conference*, 2013.

</div>

<div id="ref-rafii17">

\[318\] Z. Rafii, A. Liutkus, F.-R. Stöter, S. I. Mimilakis, and R.
Bittner, “MUSDB18, a dataset for audio source separation.” Dec-2017.

</div>

<div id="ref-raguet13">

\[319\] H. Raguet, J. Fadili, and and Gabriel Peyré, “A generalized
forward-backward splitting,” *SIAM Journal on Imaging Sciences*, vol. 6,
no. 3, pp. 1199–1226, Jul. 2013.

</div>

<div id="ref-raj04">

\[320\] B. Raj, M. L. Seltzer, and R. M. Stern, “Reconstruction of
missing features for robust speech recognition,” *Speech Communication*,
vol. 43, no. 4, pp. 275–296, Sep. 2004.

</div>

<div id="ref-raj05">

\[321\] B. Raj and P. Smaragdis, “Latent variable decomposition of
spectrograms for single channel speaker separation,” in *IEEE workshop
on applications of signal processing to audio and acoustics*, 2005.

</div>

<div id="ref-raj07">

\[322\] B. Raj, P. Smaragdis, M. Shashanka, and R. Singh, “Separating a
foreground singer from background music,” in *International symposium on
frontiers of research on speech and music*, 2007.

</div>

<div id="ref-rao11">

\[323\] V. Rao, C. Gupta, and P. Rao, “Context-aware features for
singing voice detection in polyphonic music,” in *International workshop
on adaptive multimedia retrieval*, 2011.

</div>

<div id="ref-rao10">

\[324\] V. Rao and P. Rao, “Vocal melody extraction in the presence of
pitched accompaniment in polyphonic music,” *IEEE Transactions on Audio,
Speech, and Language Processing*, vol. 18, no. 8, pp. 2145–2154, Nov.
2010.

</div>

<div id="ref-raphael08">

\[325\] C. Raphael and Y. Han, “A classifier-based approach to
score-guided music audio source separation,” *Computer Music Journal*,
vol. 32, no. 1, pp. 51–59, 2008.

</div>

<div id="ref-recht10">

\[326\] B. Recht, M. Fazel, and P. A. Parrilo, “Guaranteed minimum-rank
solutions of linear matrix equations via nuclear norm minimization,”
*SIAM Review*, vol. 52, no. 3, pp. 471–501, Aug. 2010.

</div>

<div id="ref-recht13">

\[327\] B. Recht and C. Ré, “Parallel stochastic gradient algorithms for
large-scale matrix completion,” *Mathematical Programming Computation*,
vol. 5, no. 2, pp. 201–226, Jun. 2013.

</div>

<div id="ref-recommendation2001MUSHRA">

\[328\] I. Recommendation, “Bs. 1534-1. method for the subjective
assessment of intermediate sound quality (MUSHRA),” *International
Telecommunications Union, Geneva*, 2001.

</div>

<div id="ref-rickard02">

\[329\] S. Rickard and O. Yilmaz, “On the approximate w-disjoint
orthogonality of speech,” in *IEEE international conference on
acoustics, speech, and signal processing*, 2002.

</div>

<div id="ref-rix01">

\[330\] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra,
“Perceptual evaluation of speech quality (PESQ)-a new method for
speech quality assessment of telephone networks and codecs,” in *IEEE
international conference on acoustics, speech and signal processing*,
2001.

</div>

<div id="ref-roads97">

\[331\] C. Roads, S. T. Pope, A. Piccialli, and G. de Poli, *Musical
signal processing*. Swets & Zeitlinger, 1997.

</div>

<div id="ref-robbins51">

\[332\] H. Robbins and S. Monro, “A stochastic approximation method,”
*Annals of Mathematical Statistics*, vol. 22, no. 3, pp. 400–407, Sep.
1951.

</div>

<div id="ref-rodet97">

\[333\] X. Rodet, “Musical sound signal analysis/synthesis:
Sinusoidal+Residual and elementary waveform models,” in *IEEE
time-frequency and time-scale workshop*, 1997.

</div>

<div id="ref-roma16">

\[334\] G. Roma, E. M. Grais, A. J. Simpson, I. Sobieraj, and M. D.
Plumbley, “Untwist: A new toolbox for audio source separation,” in *17th
international society on music information retrieval conference*, 2016.

</div>

<div id="ref-roweis01">

\[335\] S. T. Roweis, “One microphone source separation,” in *Advances
in neural information processing systems 13*, MIT Press, 2001, pp.
793–799.

</div>

<div id="ref-rumelhart86">

\[336\] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning
internal representations by error propagation,” in *Parallel distributed
processing: Explorations in the microstructure of cognition, vol. 1*,
MIT Press Cambridge, 1986, pp. 318–362.

</div>

<div id="ref-rumelhart862">

\[337\] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning
representations by back-propagating errors,” *Nature*, vol. 323, pp.
533–536, Oct. 1986.

</div>

<div id="ref-ryynanen082">

\[338\] M. Ryynänen and A. Klapuri, “Automatic transcription of melody,
bass line, and chords in polyphonic music,” *Computer Music Journal*,
vol. 32, no. 3, pp. 72–86, Sep. 2008.

</div>

<div id="ref-ryynanen06">

\[339\] M. Ryynänen and A. Klapuri, “Transcription of the singing melody
in polyphonic music,” in *7th international conference on music
information retrieval*, 2006.

</div>

<div id="ref-ryynanen08">

\[340\] M. Ryynänen, T. Virtanen, J. Paulus, and A. Klapuri,
“Accompaniment separation and karaoke application based on automatic
melody transcription,” in *IEEE international conference on multimedia
and expo*, 2008.

</div>

<div id="ref-sainath12">

\[341\] T. N. Sainath, B. Kingsbury, and B. Ramabhadran, “Auto-encoder
bottleneck features using deep belief networks,” in *IEEE international
conference on acoustics, speech and signal processing*, 2012.

</div>

<div id="ref-salaun14">

\[342\] Y. Salaün *et al.*, “The flexible audio source separation
toolbox version 2.0,” in *IEEE international conference on acoustics,
speech and signal processing*, 2014.

</div>

<div id="ref-salamon12">

\[343\] J. Salamon and E. Gómez, “Melody extraction from polyphonic
music signals using pitch contour characteristics,” *IEEE Transactions
on Audio, Speech, and Language Processing*, vol. 20, 2012.

</div>

<div id="ref-salamon14">

\[344\] J. Salamon, E. Gómez, D. Ellis, and G. Richard, “Melody
extraction from polyphonic music signals: Approaches, applications and
challenges,” *IEEE Signal Processing Magazine*, vol. 31, 2014.

</div>

<div id="ref-sawada13">

\[345\] H. Sawada, H. Kameoka, S. Araki, and N. Ueda, “Multichannel
extensions of non-negative matrix factorization with complex-valued
data,” *IEEE Transactions on Audio, Speech, and Language Processing*,
vol. 21, no. 5, pp. 971–982, May 2013.

</div>

<div id="ref-sawada12">

\[346\] H. Sawada, H. Kameoka, S. Araki, and N. Ueda, “Efficient
algorithms for multichannel extensions of Itakura-Saito nonnegative
matrix factorization,” in *IEEE international conference on acoustics,
speech and signal processing*, 2012.

</div>

<div id="ref-sawada11">

\[347\] H. Sawada, H. Kameoka, S. Araki, and N. Ueda, “New formulations
and efficient algorithms for multichannel NMF,” in *IEEE workshop on
applications of signal processing to audio and acoustics*, 2011.

</div>

<div id="ref-scalart96">

\[348\] P. Scalart and J. V. Filho, “Speech enhancement based on a
priori signal to noise estimation,” in *IEEE international conference on
acoustics, speech and signal processing*, 1996.

</div>

<div id="ref-schorkhuber10">

\[349\] C. Schörkhuber and A. Klapuri, “Constant-Q transform toolbox,”
in *7th sound and music computing conference*, 2010.

</div>

<div id="ref-schenker54">

\[350\] H. Schenker, *Harmony*. University of Chicago Press, 1954.

</div>

<div id="ref-schlueter16">

\[351\] J. Schlüter, “Learning to pinpoint singing voice from weakly
labeled examples,” in *17th international society for music information
retrieval conference*, 2016.

</div>

<div id="ref-schmidt09">

\[352\] M. N. Schmidt, O. Winther, and L. K. Hansen, “Bayesian
non-negative matrix factorization,” in *8th international conference on
independent component analysis and signal separation*, 2009.

</div>

<div id="ref-schwarz78">

\[353\] G. Schwarz, “Estimating the dimension of a model,” *Annals of
Statistics*, vol. 6, no. 2, pp. 461–464, Mar. 1978.

</div>

<div id="ref-sebastian16">

\[354\] J. Sebastian and H. A. Murthy, “Group delay based music source
separation using deep recurrent neural networks,” in *International
conference on signal processing and communications*, 2016.

</div>

<div id="ref-seetharaman17">

\[355\] P. Seetharaman, F. Pishdadian, and B. Pardo, “Music/voice
separation using the 2D Fourier transform,” in *IEEE workshop on
applications of signal processing to audio and acoustics*, 2017.

</div>

<div id="ref-serra89">

\[356\] X. Serra, “A system for sound analysis/transformation/synthesis
based on a deterministic plus stochastic decomposition,” Stanford
University, 1989.

</div>

<div id="ref-serra97">

\[357\] X. Serra, “Musical sound modeling with sinusoids plus noise,” in
*Musical signal processing*, Swets & Zeitlinger, 1997, pp. 91–122.

</div>

<div id="ref-shalev-shwartz02">

\[358\] S. Shalev-Shwartz, S. Dubnov, N. Friedman, and Y. Singer,
“Robust temporal and spectral modeling for query by melody,” in *25th
annual international acm sigir conference on research and development in
information retrieval*, 2002.

</div>

<div id="ref-shashanka08">

\[359\] M. Shashanka, B. Raj, and and Paris Smaragdis, “Probabilistic
latent variable models as nonnegative factorizations,” *Computational
Intelligence and Neuroscience*, vol. 2008, 2008.

</div>

<div id="ref-shi00">

\[360\] J. Shi and J. Malik, “Normalized cuts and image segmentation,”
*IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol.
22, no. 8, pp. 888–905, Aug. 2000.

</div>

<div id="ref-simpson17">

\[361\] A. J. R. Simpson, G. Roma, E. M. Grais, R. Mason, C. Hummersone,
and M. D. Plumbley, “Psychophysical evaluation of audio source
separation methods,” in *13th international conference on latent
variable analysis and signal separation*, 2017.

</div>

<div id="ref-simpson16">

\[362\] A. J. R. Simpson *et al.*, “Evaluation of audio source
separation models using hypothesis-driven non-parametric statistical
methods,” in *24th european signal processing conference*, 2016.

</div>

<div id="ref-simpson15">

\[363\] A. J. R. Simpson, G. Roma, and M. D. Plumbley, “Deep karaoke:
Extracting vocals from musical mixtures using a convolutional deep
neural network,” in *12th international conference on latent variable
analysis and signal separation*, 2015.

</div>

<div id="ref-singh10">

\[364\] R. Singh, B. Raj, and P. Smaragdis, “Latent-variable
decomposition based dereverberation of monaural and multi-channel
signals,” in *IEEE international conference on acoustics, speech and
signal processing*, 2010.

</div>

<div id="ref-sivasankaran15">

\[365\] S. Sivasankaran *et al.*, “Robust ASR using neural network based
speech enhancement and feature simulation,” in *IEEE automatic speech
recognition and understanding workshop*, 2015.

</div>

<div id="ref-slaney94">

\[366\] M. Slaney, D. Naar, and R. F. Lyon, “Auditory model inversion
for sound separation,” in *IEEE international conference on acoustics,
speech and signal processing*, 1994.

</div>

<div id="ref-smaragdis03">

\[367\] P. Smaragdis and J. C. Brown, “Non-negative matrix factorization
for polyphonic music transcription,” in *IEEE workshop on applications
of signal processing to audio and acoustics*, 2003.

</div>

<div id="ref-smaragdis09">

\[368\] P. Smaragdis and G. J. Mysore, “Separation by ‘humming’:
User-guided sound extraction from monophonic mixtures,” in *IEEE
workshop on applications of signal processing to audio and acoustics*,
2009.

</div>

<div id="ref-smaragdis06">

\[369\] P. Smaragdis and B. Raj, “Shift-invariant probabilistic latent
component analysis,” MERL, 2006.

</div>

<div id="ref-smaragdis07">

\[370\] P. Smaragdis, B. Raj, and M. Shashanka, “Supervised and
semi-supervised separation of sounds from single-channel mixtures,” in
*7th international conference on independent component analysis and
signal separation*, 2007.

</div>

<div id="ref-smith87">

\[371\] J. O. Smith and X. Serra, “PARSHL: An analysis/synthesis program
for non-harmonic sounds based on a sinusoidal representation,” in
*International computer music conference*, 1987.

</div>

<div id="ref-sofianos10">

\[372\] S. Sofianos, A. Ariyaeeinia, and R. Polfreman, “Towards
effective singing voice extraction from stereophonic recordings,” in
*IEEE international conference on acoustics, speech and signal
processing*, 2010.

</div>

<div id="ref-sofianos102">

\[373\] S. Sofianos, A. Ariyaeeinia, and R. Polfreman, “Singing voice
separation based on non-vocal independent component subtraction,” in
*13th international conference on digital audio effects*, 2010.

</div>

<div id="ref-sofianos12">

\[374\] S. Sofianos, A. Ariyaeeinia, R. Polfreman, and R. Sotudeh,
“H-semantics: A hybrid approach to singing voice separation,” *Journal
of the Audio Engineering Society*, vol. 60, no. 10, pp. 831–841, Oct.
2012.

</div>

<div id="ref-spiertz11">

\[375\] M. Spiertz and V. Gnann, “Note clustering based on 2D
source-filter modeling for underdetermined blind source separation,” in
*AES 42nd conference on semantic audio*, 2011.

</div>

<div id="ref-spiertz09">

\[376\] M. Spiertz and V. Gnann, “Source-filter based clustering for
monaural blind source separation,” in *12th international conference on
digital audio effects*, 2009.

</div>

<div id="ref-sprechmann12">

\[377\] P. Sprechmann, A. Bronstein, and G. Sapiro, “Real-time online
singing voice separation from monaural recordings using robust low-rank
modeling,” in *13th international society for music information
retrieval conference*, 2012.

</div>

<div id="ref-srivastava14">

\[378\] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R.
Salakhutdinov, “Dropout: A simple way to prevent neural networks from
overfitting,” *Journal of Machine Learning Research*, vol. 15, 2014.

</div>

<div id="ref-stoter16">

\[379\] F.-R. Stöter, A. Liutkus, R. Badeau, B. Edler, and P. Magron,
“Common fate model for unison source separation,” in *IEEE
international conference on acoustics, speech and signal processing*,
2016.

</div>

<div id="ref-stoter14">

\[380\] F.-R. Stöter, S. Bayer, and B. Edler, “Unison source
separation,” in *17th international conference on digital audio
effects*, 2014.

</div>

<div id="ref-sturmel12">

\[381\] N. Sturmel *et al.*, “Linear mixing models for active listening
of music productions in realistic studio conditions,” in *132nd aes
convention*, 2012.

</div>

<div id="ref-sun13">

\[382\] D. L. Sun and G. J. Mysore, “Universal speech models for speaker
independent single channel source separation,” in *IEEE international
conference on acoustics, speech and signal processing*, 2013.

</div>

<div id="ref-tachibana12">

\[383\] H. Tachibana, H. Kameoka, N. Ono, and S. Sagayama, “Comparative
evaluations of multiple harmonic/percussive sound separation techniques
based on anisotropic smoothness of spectrogram,” in *IEEE international
conference on acoustics, speech and signal processing*, 2012.

</div>

<div id="ref-tachibana16">

\[384\] H. Tachibana, N. Ono, and S. Sagayama, “A real-time
audio-to-audio karaoke generation system for monaural recordings based
on singing voice suppression and key conversion techniques,” *Journal of
Information Processing*, vol. 24, no. 3, pp. 470–482, May 2016.

</div>

<div id="ref-tachibana14">

\[385\] H. Tachibana, N. Ono, and S. Sagayama, “Singing voice
enhancement in monaural music signals based on two-stage
harmonic/percussive sound separation on multiple resolution
spectrograms,” *IEEE/ACM Transactions on Audio, Speech and Language
Processing*, vol. 22, no. 1, pp. 228–237, Jan. 2014.

</div>

<div id="ref-tachibana10">

\[386\] H. Tachibana, T. Ono, N. Ono, and S. Sagayama, “Melody line
estimation in homophonic music audio signals based on
temporal-variability of melodic source,” in *IEEE international
conference on acoustics, speech and signal processing*, 2010.

</div>

<div id="ref-takahashi17">

\[387\] N. Takahashi and Y. Mitsufuji, “Multi-scale multi-band densenets
for audio source separation,” in *IEEE workshop on applications of
signal processing to audio and acoustics*, 2017.

</div>

<div id="ref-talmon11">

\[388\] R. Talmon, I. Cohen, and S. Gannot, “Transient noise reduction
using nonlocal diffusion filters,” *IEEE/ACM Transactions on Audio,
Speech and Language Processing*, vol. 19, no. 6, pp. 1584–1599, Aug.
2011.

</div>

<div id="ref-terhardt79">

\[389\] E. Terhardt, “Calculating virtual pitch,” *Hearing Research*,
vol. 1, no. 2, pp. 155–182, Mar. 1979.

</div>

<div id="ref-tikhonov63">

\[390\] A. N. Tikhonov, “Solution of incorrectly formulated problems and
the regularization method,” *Soviet Mathematics*, vol. 4, pp. 1035–1038,
1963.

</div>

<div id="ref-tsai04">

\[391\] W.-H. Tsai, D. Rogers, and H.-M. Wang, “Blind clustering of
popular music recordings based on singer voice characteristics,”
*Computer Music Journal*, vol. 28, no. 3, pp. 68–78, 2004.

</div>

<div id="ref-uhlich15">

\[392\] S. Uhlich, F. Giron, and Y. Mitsufuji, “Deep neural network
based instrument extraction from music,” in *IEEE international
conference on acoustics, speech and signal processing*, 2015.

</div>

<div id="ref-uhlich17">

\[393\] S. Uhlich *et al.*, “Improving music source separation based on
deep neural networks through data augmentation and network blending,” in
*IEEE international conference on acoustics, speech and signal
processing*, 2017.

</div>

<div id="ref-umap15">

\[394\] P. K. Umap, K. B. Chaudhari, and M. A. Joshi, “Unsupervised
singing voice separation from music accompaniment using robust principal
componenet analysis,” in *International conference on industrial
instrumentation and control*, 2015.

</div>

<div id="ref-vaneph16">

\[395\] A. Vaneph, E. McNeil, and F. Rigaud, “An automated source
separation technology and its practical applications,” in *140th aes
convention*, 2016.

</div>

<div id="ref-varga93">

\[396\] A. Varga and H. J. Steeneken, “Assessment for automatic speech
recognition: II. NOISEX-92: A database and an experiment to study the
effect of additive noise on speech recognition systems,” *Speech
Communication*, vol. 12, no. 3, pp. 247–251, Jul. 1993.

</div>

<div id="ref-vanveen97">

\[397\] B. V. Veen and K. M. Buckley, “Beamforming techniques for
spatial filtering,” in *The digital signal processing handbook*, CRC
Press, 1997, pp. 1–22.

</div>

<div id="ref-vembu05">

\[398\] S. Vembu and S. Baumann, “Separation of vocals from polyphonic
audio recordings,” in *6th international conference on music information
retrieval*, 2005.

</div>

<div id="ref-venkataramani14">

\[399\] S. Venkataramani, N. Nayak, P. Rao, and R. Velmurugan, “Vocal
separation using singer-vowel priors obtained from polyphonic audio,” in
*15th international society for music information retrieval conference*,
2014.

</div>

<div id="ref-vergin99">

\[400\] R. Vergin, D. O’Shaughnessy, and A. Farhat, “Generalized mel
frequency cepstral coefficients for large-vocabulary speaker-independent
continuous-speech recognition,” *IEEE Transactions on Audio, Speech, and
Language Processing*, vol. 7, no. 5, pp. 525–532, Sep. 1999.

</div>

<div id="ref-vincent08">

\[401\] E. Vincent, N. Bertin, and R. Badeau, “Harmonic and inharmonic
nonnegative matrix factorization for polyphonic pitch transcription,” in
*IEEE international conference on acoustics, speech and signal
processing*, 2008.

</div>

<div id="ref-vincent09">

\[402\] E. Vincent, S. Araki, and P. Bofill, “The 2008 signal separation
evaluation campaign: A community-based approach to large-scale
evaluation,” in *8th international conference on independent component
analysis and signal separation*, 2009.

</div>

<div id="ref-vincent12">

\[403\] E. Vincent *et al.*, “The signal separation evaluation campaign
(2007-2010): Achievements and remaining challenges,” *Signal
Processing*, vol. 92, no. 8, pp. 1928–1936, Aug. 2012.

</div>

<div id="ref-vincent122">

\[404\] E. Vincent, “Improved perceptual metrics for the evaluation of
audio source separation,” in *10th international conference on latent
variable analysis and signal separation*, 2012.

</div>

<div id="ref-vincent14">

\[405\] E. Vincent, N. Bertin, R. Gribonval, and F. Bimbot, “From blind
to guided audio source separation: How models and side information can
improve the separation of sound,” *IEEE Signal Processing Magazine*,
vol. 31, no. 3, pp. 107–115, May 2014.

</div>

<div id="ref-vincent06">

\[406\] E. Vincent, R. Gribonval, and C. Févotte, “Performance
measurement in blind audio source separation,” *IEEE Transactions on
Audio, Speech, and Language Processing*, vol. 14, no. 4, pp. 1462–1469,
Jul. 2006.

</div>

<div id="ref-vincent10">

\[407\] E. Vincent, M. Jafari, S. Abdallah, M. Plumbley, and M. Davies,
“Probabilistic modeling paradigms for audio source separation,” in
*Machine audition: Principles, algorithms and systems*, IGI Global,
2010, pp. 162–185.

</div>

<div id="ref-vincent062">

\[408\] E. Vincent, M. Jafari, and M. Plumbley, “Preliminary guidelines
for subjective evaluation of audio source separation algorithms,” in
*ICA research network international workshop*, 2006.

</div>

<div id="ref-vincent07">

\[409\] E. Vincent, H. Sawada, P. Bofill, S. Makino, and J. P. Rosca,
“First stereo audio source separation evaluation campaign: Data,
algorithms and results,” in *7th international conference on independent
component analysis and blind source separation*, 2007.

</div>

<div id="ref-vincentp10">

\[410\] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A.
Manzagol, “Stacked denoising autoencoders: Learning useful
representations in a deep network with a local denoising criterion,”
*Journal of Machine Learning Research*, vol. 11, pp. 3371–3408, Dec.
2010.

</div>

<div id="ref-vincent18">

\[411\] E. Vincent, T. Virtanen, and S. Gannot, *Audio source separation
and speech enhancement*. Wiley, 2018.

</div>

<div id="ref-viterbi2006">

\[412\] A. J. Viterbi, “A personal history of the Viterbi algorithm,”
*IEEE Signal Processing Magazine*, vol. 23, no. 4, pp. 120–142, 2006.

</div>

<div id="ref-MTGMASSdb">

\[413\] M. Vinyes, “MTG MASS database.” 2008.

</div>

<div id="ref-vinyes06">

\[414\] M. Vinyes, J. Bonada, and A. Loscos, “Demixing commercial music
productions via human-assisted time-frequency masking,” in *120th aes
convention*, 2006.

</div>

<div id="ref-virtanen07">

\[415\] T. Virtanen, “Monaural sound source separation by nonnegative
matrix factorization with temporal continuity and sparseness criteria,”
*IEEE Transactions on Audio, Speech, and Language Processing*, vol. 15,
no. 3, pp. 1066–1074, Mar. 2007.

</div>

<div id="ref-virtanen08">

\[416\] T. Virtanen, A. Mesaros, and M. Ryynänen, “Combining pitch-based
inference and non-negative spectrogram factorization in separating
vocals from polyphonic music,” in *ISCA tutorial and research workshop
on statistical and perceptual audition*, 2008.

</div>

<div id="ref-wang95">

\[417\] A. L. Wang, “Instantaneous and frequency-warped techniques for
source separation and signal parametrization,” in *IEEE workshop on
applications of signal processing to audio and acoustics*, 1995.

</div>

<div id="ref-wang94">

\[418\] A. L. Wang, “Instantaneous and frequency-warped techniques for
auditory source separation,” PhD thesis, Stanford University, 1994.

</div>

<div id="ref-wang06">

\[419\] D. Wang and G. J. Brown, *Computational auditory scene analysis:
Principles, algorithms, and applications*. Wiley-IEEE Press, 2006.

</div>

<div id="ref-wang11">

\[420\] Y. Wang and Z. Ou, “Combining HMM-based melody extraction and
NMF-based soft masking for separating voice and accompaniment from
monaural audio,” in *IEEE international conference on acoustics, speech
and signal processing*, 2011.

</div>

<div id="ref-wang09">

\[421\] Z. Wang and A. C. Bovik, “Mean squared error: Love it or leave
it? A new look at signal fidelity measures,” *IEEE Signal Processing
Magazine*, vol. 26, no. 1, pp. 98–117, Jan. 2009.

</div>

<div id="ref-watanabe16">

\[422\] T. Watanabe, T. Fujisawa, and M. Ikehara, “Vocal separation
using improved robust principal component analysis and post-processing,”
in *IEEE 59th international midwest symposium on circuits and systems*,
2016.

</div>

<div id="ref-weniger14">

\[423\] F. Weninger, J. R. Hershey, J. L. Roux, and B. Schuller,
“Discriminatively trained recurrent neural networks for single-channel
speech separation,” in *IEEE global conference on signal and information
processing*, 2014.

</div>

<div id="ref-werner17">

\[424\] N. Werner, S. Balke, F.-R. Stöter, M. Müller, and B. Edler,
“Trackswitch.js: A versatile web-based audio player for presenting
scientifc results,” in *3rd web audio conference, london, uk*, 2017.

</div>

<div id="ref-wiener1975">

\[425\] N. Wiener, “Extrapolation, interpolation, and smoothing of
stationary time series,” 1975.

</div>

<div id="ref-wolf16">

\[426\] G. Wolf, S. Mallat, and S. Shamma, “Rigid motion model for audio
source separation,” *IEEE Transactions on Signal Processing*, vol. 64,
no. 7, pp. 1822–1831, Apr. 2016.

</div>

<div id="ref-wolf14">

\[427\] G. Wolf, S. Mallat, and S. Shamma, “Audio source separation with
time-frequency velocities,” in *IEEE international workshop on machine
learning for signal processing*, 2014.

</div>

<div id="ref-wu03">

\[428\] M. Wu, D. Wang, and G. J. Brown, “A multipitch tracking
algorithm for noisy speech,” *IEEE Transactions on Audio, Speech, and
Language Processing*, vol. 11, no. 3, pp. 229–241, May 2003.

</div>

<div id="ref-xu14">

\[429\] J. Xu, X. Li, Y. Hao, and G. Yang, “Source separation improves
music emotion recognition,” in *4th acm international conference on
multimedia retrieval*, 2014.

</div>

<div id="ref-yilmaz04">

\[430\] Ö. Yilmaz and S. Rickard, “Blind separation of speech mixtures
via time-frequency masking,” *IEEE Transactions on Signal Processing*,
vol. 52, no. 7, pp. 1830–1847, Jul. 2004.

</div>

<div id="ref-yang14">

\[431\] P.-K. Yang, C.-C. Hsu, and J.-T. Chien, “Bayesian singing-voice
separation,” in *15th international society for music information
retrieval conference*, 2014.

</div>

<div id="ref-yang13">

\[432\] Y.-H. Yang, “Low-rank representation of both singing voice and
music accompaniment via learned dictionaries,” in *14th international
society for music information retrieval conference*, 2013.

</div>

<div id="ref-yang12">

\[433\] Y.-H. Yang, “On sparse and low-rank matrix decomposition for
singing voice separation,” in *20th acm international conference on
multimedia*, 2012.

</div>

<div id="ref-yegnanarayana91">

\[434\] B. Yegnanarayana, H. A. Murthy, and V. R. Ramachandran,
“Processing of noisy speech using modified group delay functions,” in
*IEEE international conference on acoustics, speech and signal
processing*, 1991.

</div>

<div id="ref-fanoyela17">

\[435\] D. F. Yela, S. Ewert, D. FitzGerald, and M. Sandler,
“Interference reduction in music recordings combining kernel additive
modelling and non-negative matrix factorization,” in *IEEE international
conference on acoustics, speech and signal processing*, 2017.

</div>

<div id="ref-yen15">

\[436\] F. Yen, M.-C. Huang, and T.-S. Chi, “A two-stage singing voice
separation algorithm using spectro-temporal modulation features,” in
*Interspeech*, 2015.

</div>

<div id="ref-yen14">

\[437\] F. Yen, Y.-J. Luo, and T.-S. Chi, “Singing voice separation
using spectro-temporal modulation features,” in *15th international
society for music information retrieval conference*, 2014.

</div>

<div id="ref-yoo10">

\[438\] J. Yoo, M. Kim, K. Kang, and S. Choi, “Nonnegative matrix
partial co-factorization for drum source separation,” in *IEEE
international conference on acoustics, speech and signal processing*,
2010.

</div>

<div id="ref-yuan06">

\[439\] M. Yuan and Y. Lin, “Model selection and estimation in
regression with grouped variables,” *Journal of the Royal Statistical
Society Series B*, vol. 68, no. 1, pp. 49–67, Dec. 2006.

</div>

<div id="ref-zhang97">

\[440\] F. Zhang, “Quaternions and matrices of quaternions,” *Linear
Algebra and its Applications*, vol. 251, 1997.

</div>

<div id="ref-zhou14">

\[441\] L. Zhang, Z. Chen, M. Zheng, and X. He, “Nonnegative matrix and
tensor factorizations: An algorithmic perspective,” *IEEE Signal
Processing Magazine*, vol. 31, no. 3, pp. 54–65, May 2014.

</div>

<div id="ref-zhang11">

\[442\] L. Zhang, Z. Chen, M. Zheng, and X. He, “Robust non-negative
matrix factorization,” *Frontiers of Electrical Electronic Engineering
China*, vol. 6, no. 2, pp. 192–200, Jun. 2011.

</div>

<div id="ref-zhang15">

\[443\] X. Zhang, W. Li, and B. Zhu, “Latent time-frequency component
analysis: A novel pitch-based approach for singing voice separation,” in
*IEEE international conference on acoustics, speech and signal
processing*, 2015.

</div>

<div id="ref-zhang06">

\[444\] Y.-G. Zhang and C.-S. Zhang, “Separation of music signals by
harmonic structure modeling,” in *Advances in neural information
processing systems 18*, MIT Press, 2006, pp. 1617–1624.

</div>

<div id="ref-zhang05">

\[445\] Y.-G. Zhang and C.-S. Zhang, “Separation of voice and music by
harmonic structure stability analysis,” in *IEEE international
conference on multimedia and expo*, 2005.

</div>

<div id="ref-zhang03">

\[446\] Y.-G. Zhang, C.-S. Zhang, and S. Wang, “Clustering in knowledge
embedded space,” in *Machine learning: ECML 2003*, Springer Berlin
Heidelberg, 2003, pp. 480–491.

</div>

<div id="ref-zhao14">

\[447\] R. Zhao, S. Lee, D.-Y. Huang, and M. Dong, “Soft constrained
leading voice separation with music score guidance,” in *9th
international symposium on chinese spoken language*, 2014.

</div>

<div id="ref-zhu15">

\[448\] B. Zhu, W. Li, and L. Li, “Towards solving the bottleneck of
pitch-based singing voice separation,” in *23rd acm international
conference on multimedia*, 2015.

</div>

<div id="ref-zhu13">

\[449\] B. Zhu, W. Li, R. Li, and X. Xue, “Multi-stage non-negative
matrix factorization for monaural singing voice separation,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 21, no.
10, pp. 2096–2107, Oct. 2013.

</div>

<div id="ref-zolzer11">

\[450\] U. Zölzer, *DAFX - digital audio effects*. Wiley, 2011.

</div>

<div id="ref-zwicker13">

\[451\] E. Zwicker and H. Fastl, *Psychoacoustics: Facts and models*.
Springer-Verlag Berlin Heidelberg, 2013.

</div>

<div id="ref-feng022">

\[452\] Y. Feng, Y. Zhuang, and Y. Pan, “Popular song retrieval based on
singing matching,” in *IEEE pacific-rim conference on multimedia*, 2002.

</div>

</div>
