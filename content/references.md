# References

<div id="refs" class="references">

<div id="ref-kalakota00">

\[1\] R. Kalakota and M. Robinson, *E-business 2.0: Roadmap for
success*. Addison-Wesley Professional, 2000.

</div>
  
<div id="ref-lam01">

\[2\] C. K. Lam and B. C. Tan, “The Internet is changing the music
industry,” *Communications of the ACM*, vol. 44, no. 8, pp. 62–68, 2001.

</div>

<div id="ref-common10">

\[3\] P. Common and C. Jutten, *Handbook of blind source separation*.
Academic Press, 2010.

</div>

<div id="ref-naik14">

\[4\] G. R. Naik and W. Wang, *Blind source separation*. Springer-Verlag
Berlin Heidelberg, 2014.

</div>

<div id="ref-hyvarinen99">

\[5\] A. Hyvärinen, “Fast and robust fixed-point algorithm for
independent component analysis,” *IEEE Transactions on Neural Networks*,
vol. 10, no. 3, pp. 626–634, May 1999.

</div>

<div id="ref-hyvarinen00">

\[6\] A. Hyvärinen and E. Oja, “Independent component analysis:
Algorithms and applications,” *Neural Networks*, vol. 13, nos. 4-5, pp.
411–430, Jun. 2000.

</div>

<div id="ref-makino07">

\[7\] S. Makino, T.-W. Lee, and H. Sawada, *Blind speech separation*.
Springer Netherlands, 2007.

</div>

<div id="ref-vincent18">

\[8\] E. Vincent, T. Virtanen, and S. Gannot, *Audio source separation
and speech enhancement*. Wiley, 2018.

</div>

<div id="ref-loizou13">

\[9\] P. C. Loizou, *Speech enhancement: Theory and practice*. CRC
Press, 1990.

</div>

<div id="ref-liutkus13">

\[10\] A. Liutkus, J.-L. Durrieu, L. Daudet, and G. Richard, “An
overview of informed audio source separation,” in *14th international
workshop on image analysis for multimedia interactive services*, 2013.

</div>

<div id="ref-vincent14">

\[11\] E. Vincent, N. Bertin, R. Gribonval, and F. Bimbot, “From blind
to guided audio source separation: How models and side information can
improve the separation of sound,” *IEEE Signal Processing Magazine*,
vol. 31, no. 3, pp. 107–115, May 2014.

</div>

<div id="ref-zolzer11">

\[12\] U. Zölzer, *DAFX - digital audio effects*. Wiley, 2011.

</div>

<div id="ref-muller2015">

\[13\] M. Müller, *Fundamentals of music processing: Audio, analysis,
algorithms, applications*. Springer, 2015.

</div>

<div id="ref-jaynes2003probability">

\[14\] E. T. Jaynes, *Probability theory: The logic of science*.
Cambridge university press, 2003.

</div>

<div id="ref-cappe2005">

\[15\] O. Cappé, E. Moulines, and T. Ryden, *Inference in hidden markov
models (springer series in statistics)*. Secaucus, NJ, USA:
Springer-Verlag New York, Inc., 2005.

</div>

<div id="ref-mcaulay86">

\[16\] R. J. McAulay and T. F. Quatieri, “Speech analysis/synthesis
based on a sinusoidal representation,” *IEEE Transactions on Audio,
Speech, and Language Processing*, vol. 34, no. 4, pp. 744–754, Aug.
1986.

</div>

<div id="ref-rickard02">

\[17\] S. Rickard and O. Yilmaz, “On the approximate w-disjoint
orthogonality of speech,” in *IEEE international conference on
acoustics, speech, and signal processing*, 2002.

</div>

<div id="ref-boll1979">

\[18\] S. Boll, “Suppression of acoustic noise in speech using spectral
subtraction,” *IEEE Transactions on acoustics, speech, and signal
processing*, vol. 27, no. 2, pp. 113–120, 1979.

</div>

<div id="ref-wiener1975">

\[19\] N. Wiener, “Extrapolation, interpolation, and smoothing of
stationary time series,” 1975.

</div>

<div id="ref-liutkus15c">

\[20\] A. Liutkus and R. Badeau, “Generalized Wiener filtering with
fractional power spectrograms,” in *IEEE international conference on
acoustics, speech and signal processing*, 2015.

</div>

<div id="ref-fant70">

\[21\] G. Fant, *Acoustic theory of speech production*. Walter de
Gruyter, 1970.

</div>

<div id="ref-bogert1963">

\[22\] B. P. Bogert, M. J. R. Healy, and J. W. Tukey, “The quefrency
alanysis of time series for echoes: Cepstrum pseudo-autocovariance,
cross-cepstrum, and saphe cracking,” *Proceedings of a symposium on time
series analysis*, pp. 209–243, 1963.

</div>

<div id="ref-noll64">

\[23\] A. M. Noll, “Short-time spectrum and ‘cepstrum’ techniques for
vocal-pitch detection,” *Journal of the Acoustical Society of America*,
vol. 36, no. 2, pp. 296–302, 1964.

</div>

<div id="ref-noll67">

\[24\] A. M. Noll, “Cepstrum pitch determination,” *Journal of the
Acoustical Society of America*, vol. 41, no. 2, pp. 293–309, 1967.

</div>

<div id="ref-david80">

\[25\] S. B. Davis and P. Mermelstein, “Comparison of parametric
representations for monosyllabic word recognition in continuously spoken
sentences,” *IEEE Transactions on Audio, Speech, and Language
Processing*, vol. 28, no. 4, pp. 357–366, Aug. 1980.

</div>

<div id="ref-oppenheim69">

\[26\] A. V. Oppenheim, “Speech analysis-synthesis system based on
homomorphic filtering,” *Journal of the Acoustical Society of America*,
vol. 45, no. 2, pp. 458–465, 1969.

</div>

<div id="ref-durrett2010probability">

\[27\] R. Durrett, *Probability: Theory and examples*. Cambridge
university press, 2010.

</div>

<div id="ref-schwarz78">

\[28\] G. Schwarz, “Estimating the dimension of a model,” *Annals of
Statistics*, vol. 6, no. 2, pp. 461–464, Mar. 1978.

</div>

<div id="ref-rabiner89">

\[29\] L. R. Rabiner, “A tutorial on hidden Markov models and selected
applications in speech recognition,” *Proceedings of the IEEE*, vol. 77,
no. 2, pp. 257–286, Feb. 1989.

</div>

<div id="ref-viterbi2006">

\[30\] A. J. Viterbi, “A personal history of the Viterbi algorithm,”
*IEEE Signal Processing Magazine*, vol. 23, no. 4, pp. 120–142, 2006.

</div>

<div id="ref-bishop96">

\[31\] C. Bishop, *Neural networks for pattern recognition*. Clarendon
Press, 1996.

</div>

<div id="ref-dempster77">

\[32\] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood
from incomplete data via the EM algorithm,” *Journal of the Royal
Statistical Society*, vol. 39, no. 1, pp. 1–38, 1977.

</div>

<div id="ref-salamon14">

\[33\] J. Salamon, E. Gómez, D. Ellis, and G. Richard, “Melody
extraction from polyphonic music signals: Approaches, applications and
challenges,” *IEEE Signal Processing Magazine*, vol. 31, 2014.

</div>

<div id="ref-miller73">

\[34\] N. J. Miller, “Removal of noise from a voice signal by
synthesis,” Utah University, 1973.

</div>

<div id="ref-oppenheim68">

\[35\] A. V. Oppenheim and R. W. Schafer, “Homomorphic analysis of
speech,” *IEEE Transactions on Audio and Electroacoustics*, vol. 16, no.
2, pp. 221–226, Jun. 1968.

</div>

<div id="ref-maher89">

\[36\] R. C. Maher, “An approach for the separation of voices in
composite musical signals,” PhD thesis, University of Illinois at
Urbana-Champaign, 1989.

</div>

<div id="ref-wang94">

\[37\] A. L. Wang, “Instantaneous and frequency-warped techniques for
auditory source separation,” PhD thesis, Stanford University, 1994.

</div>

<div id="ref-wang95">

\[38\] A. L. Wang, “Instantaneous and frequency-warped techniques for
source separation and signal parametrization,” in *IEEE workshop on
applications of signal processing to audio and acoustics*, 1995.

</div>

<div id="ref-meron98">

\[39\] Y. Meron and K. Hirose, “Separation of singing and piano sounds,”
in *5th international conference on spoken language processing*, 1998.

</div>

<div id="ref-quatieri92">

\[40\] T. F. Quatieri, “Shape invariant time-scale and pitch
modification of speech,” *IEEE Transactions on Signal Processing*, vol.
40, no. 3, pp. 497–510, Mar. 1992.

</div>

<div id="ref-ben-shalom04">

\[41\] A. Ben-Shalom and S. Dubnov, “Optimal filtering of an instrument
sound in a mixed recording given approximate pitch prior,” in
*International computer music conference*, 2004.

</div>

<div id="ref-shalev-shwartz02">

\[42\] S. Shalev-Shwartz, S. Dubnov, N. Friedman, and Y. Singer, “Robust
temporal and spectral modeling for query by melody,” in *25th annual
international acm sigir conference on research and development in
information retrieval*, 2002.

</div>

<div id="ref-serra97">

\[43\] X. Serra, “Musical sound modeling with sinusoids plus noise,” in
*Musical signal processing*, Swets & Zeitlinger, 1997, pp. 91–122.

</div>

<div id="ref-vanveen97">

\[44\] B. V. Veen and K. M. Buckley, “Beamforming techniques for spatial
filtering,” in *The digital signal processing handbook*, CRC Press,
1997, pp. 1–22.

</div>

<div id="ref-zhang05">

\[45\] Y.-G. Zhang and C.-S. Zhang, “Separation of voice and music by
harmonic structure stability analysis,” in *IEEE international
conference on multimedia and expo*, 2005.

</div>

<div id="ref-zhang06">

\[46\] Y.-G. Zhang and C.-S. Zhang, “Separation of music signals by
harmonic structure modeling,” in *Advances in neural information
processing systems 18*, MIT Press, 2006, pp. 1617–1624.

</div>

<div id="ref-terhardt79">

\[47\] E. Terhardt, “Calculating virtual pitch,” *Hearing Research*,
vol. 1, no. 2, pp. 155–182, Mar. 1979.

</div>

<div id="ref-zhang03">

\[48\] Y.-G. Zhang, C.-S. Zhang, and S. Wang, “Clustering in knowledge
embedded space,” in *Machine learning: ECML 2003*, Springer Berlin
Heidelberg, 2003, pp. 480–491.

</div>

<div id="ref-fujihara05">

\[49\] H. Fujihara, T. Kitahara, M. Goto, K. Komatani, T. Ogata, and H.
G. Okuno, “Singer identification based on accompaniment sound reduction
and reliable frame selection,” in *6th international conference on music
information retrieval*, 2005.

</div>

<div id="ref-fujihara10">

\[50\] H. Fujihara, M. Goto, T. Kitahara, and H. G. Okuno, “A modeling
of singing voice robust to accompaniment sounds and its application to
singer identification and vocal-timbre-similarity-based music
information retrieval,” *IEEE Transactions on Audio, Speech, and
Language Processing*, vol. 18, no. 3, pp. 638–648, Mar. 2010.

</div>

<div id="ref-goto04">

\[51\] M. Goto, “A real-time music-scene-description system:
Predominant-F0 estimation for detecting melody and bass lines in
real-world audio signals,” *Speech Communication*, vol. 43, no. 4, pp.
311–329, Sep. 2004.

</div>

<div id="ref-moorer05">

\[52\] J. A. Moorer, “Signal processing aspects of computer music: A
survey,” *Proceedings of the IEEE*, vol. 65, no. 8, pp. 1108–1137, Aug.
2005.

</div>

<div id="ref-mesaros07">

\[53\] A. Mesaros, T. Virtanen, and A. Klapuri, “Singer identification
in polyphonic music using vocal separation and pattern recognition
methods,” in *7th international conference on music information
retrieval*, 2007.

</div>

<div id="ref-ryynanen06">

\[54\] M. Ryynänen and A. Klapuri, “Transcription of the singing melody
in polyphonic music,” in *7th international conference on music
information retrieval*, 2006.

</div>

<div id="ref-duan08">

\[55\] Z. Duan, Y.-F. Zhang, C.-S. Zhang, and Z. Shi, “Unsupervised
single-channel music source separation by average harmonic structure
modeling,” *IEEE Transactions on Audio, Speech, and Language
Processing*, vol. 16, no. 4, pp. 766–778, May 2008.

</div>

<div id="ref-rodet97">

\[56\] X. Rodet, “Musical sound signal analysis/synthesis:
Sinusoidal+Residual and elementary waveform models,” in *IEEE
time-frequency and time-scale workshop*, 1997.

</div>

<div id="ref-smith87">

\[57\] J. O. Smith and X. Serra, “PARSHL: An analysis/synthesis program
for non-harmonic sounds based on a sinusoidal representation,” in
*International computer music conference*, 1987.

</div>

<div id="ref-slaney94">

\[58\] M. Slaney, D. Naar, and R. F. Lyon, “Auditory model inversion for
sound separation,” in *IEEE international conference on acoustics,
speech and signal processing*, 1994.

</div>

<div id="ref-lagrange07">

\[59\] M. Lagrange and G. Tzanetakis, “Sound source tracking and
formation using normalized cuts,” in *IEEE international conference on
acoustics, speech and signal processing*, 2007.

</div>

<div id="ref-lagrange08">

\[60\] M. Lagrange, L. G. Martins, J. Murdoch, and G. Tzanetakis,
“Normalized cuts for predominant melodic source separation,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 16, no. 2,
pp. 278–290, Feb. 2008.

</div>

<div id="ref-shi00">

\[61\] J. Shi and J. Malik, “Normalized cuts and image segmentation,”
*IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol.
22, no. 8, pp. 888–905, Aug. 2000.

</div>

<div id="ref-ryynanen08">

\[62\] M. Ryynänen, T. Virtanen, J. Paulus, and A. Klapuri,
“Accompaniment separation and karaoke application based on automatic
melody transcription,” in *IEEE international conference on multimedia
and expo*, 2008.

</div>

<div id="ref-ryynanen082">

\[63\] M. Ryynänen and A. Klapuri, “Automatic transcription of melody,
bass line, and chords in polyphonic music,” *Computer Music Journal*,
vol. 32, no. 3, pp. 72–86, Sep. 2008.

</div>

<div id="ref-ding97">

\[64\] Y. Ding and X. Qian, “Processing of musical tones using a
combined quadratic polynomial-phase sinusoid and residual (QUASAR)
signal model,” *Journal of the Audio Engineering Society*, vol. 45, no.
7/8, pp. 571–584, Jul. 1997.

</div>

<div id="ref-li06">

\[65\] Y. Li and D. Wang, “Singing voice separation from monaural
recordings,” in *7th international conference on music information
retrieval*, 2006.

</div>

<div id="ref-li07">

\[66\] Y. Li and D. Wang, “Separation of singing voice from music
accompaniment for monaural recordings,” *IEEE Transactions on Audio,
Speech, and Language Processing*, vol. 15, no. 4, pp. 1475–1487, May
2007.

</div>

<div id="ref-duxbury03">

\[67\] C. Duxbury, J. P. Bello, M. Davies, and M. Sandler, “Complex
domain onset detection for musical signals,” in *6th international
conference on digital audio effects*, 2003.

</div>

<div id="ref-li05">

\[68\] Y. Li and D. Wang, “Detecting pitch of singing voice in
polyphonic audio,” in *IEEE international conference on acoustics,
speech and signal processing*, 2005.

</div>

<div id="ref-wu03">

\[69\] M. Wu, D. Wang, and G. J. Brown, “A multipitch tracking algorithm
for noisy speech,” *IEEE Transactions on Audio, Speech, and Language
Processing*, vol. 11, no. 3, pp. 229–241, May 2003.

</div>

<div id="ref-hu02">

\[70\] G. Hu and D. Wang, “Monaural speech segregation based on pitch
tracking and amplitude modulation,” *IEEE Transactions on Neural
Networks*, vol. 15, no. 5, pp. 1135–1150, Sep. 2002.

</div>

<div id="ref-han07">

\[71\] Y. Han and C. Raphael, “Desoloing monaural audio using mixture
models,” in *7th international conference on music information
retrieval*, 2007.

</div>

<div id="ref-roweis01">

\[72\] S. T. Roweis, “One microphone source separation,” in *Advances in
neural information processing systems 13*, MIT Press, 2001, pp. 793–799.

</div>

<div id="ref-hsu08">

\[73\] C.-L. Hsu, J.-S. R. Jang, and T.-L. Tsai, “Separation of singing
voice from music accompaniment with unvoiced sounds reconstruction for
monaural recordings,” in *AES 125th convention*, 2008.

</div>

<div id="ref-hsu10">

\[74\] C.-L. Hsu and J.-S. R. Jang, “On the improvement of singing voice
separation for monaural recordings using the MIR-1K dataset,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 18, no. 2,
pp. 310–319, Feb. 2010.

</div>

<div id="ref-dressler062">

\[75\] K. Dressler, “Sinusoidal extraction using an efficient
implementation of a multi-resolution FFT,” in *9th international
conference on digital audio effects*, 2006.

</div>

<div id="ref-scalart96">

\[76\] P. Scalart and J. V. Filho, “Speech enhancement based on a priori
signal to noise estimation,” in *IEEE international conference on
acoustics, speech and signal processing*, 1996.

</div>

<div id="ref-raphael08">

\[77\] C. Raphael and Y. Han, “A classifier-based approach to
score-guided music audio source separation,” *Computer Music Journal*,
vol. 32, no. 1, pp. 51–59, 2008.

</div>

<div id="ref-breiman84">

\[78\] L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen,
*Classification and regression trees*. Chapman; Hall/CRC, 1984.

</div>

<div id="ref-cano09">

\[79\] E. Cano and C. Cheng, “Melody line detection and source
separation in classical saxophone recordings,” in *12th international
conference on digital audio effects*, 2009.

</div>

<div id="ref-grollmisch11">

\[80\] S. Grollmisch, E. Cano, and C. Dittmar, “Songs2See: Learn to play
by playing,” in *AES 41st conference: Audio for games*, 2011, pp. P2–3.

</div>

<div id="ref-dittmar12">

\[81\] C. Dittmar, E. Cano, J. Abeßer, and S. Grollmisch, “Music
information retrieval meets music education,” in *Multimodal music
processing*, Dagstuhl Publishing, 2012, pp. 95–120.

</div>

<div id="ref-cano12">

\[82\] E. Cano, C. Dittmar, and G. Schuller, “Efficient implementation
of a system for solo and accompaniment separation in polyphonic music,”
in *20th european signal processing conference*, 2012.

</div>

<div id="ref-dressler11">

\[83\] K. Dressler, “Pitch estimation by the pair-wise evaluation of
spectral peaks,” in *42nd aes conference on semantic audio*, 2011.

</div>

<div id="ref-cano13">

\[84\] E. Cano, C. Dittmar, and G. Schuller, “Re-thinking sound
separation: Prior information and additivity constraints in separation
algorithms,” in *16th international conference on digital audio
effects*, 2013.

</div>

<div id="ref-cano14">

\[85\] E. Cano, G. Schuller, and C. Dittmar, “Pitch-informed solo and
accompaniment separation towards its use in music education
applications,” *EURASIP Journal on Advances in Signal Processing*, vol.
2014, no. 23, Sep. 2014.

</div>

<div id="ref-bosch12">

\[86\] J. J. Bosch, K. Kondo, R. Marxer, and J. Janer, “Score-informed
and timbre independent lead instrument separation in real-world
scenarios,” in *20th european signal processing conference*, 2012.

</div>

<div id="ref-marxer12">

\[87\] R. Marxer, J. Janer, and J. Bonada, “Low-latency instrument
separation in polyphonic audio using timbre models,” in *10th
international conference on latent variable analysis and signal
separation*, 2012.

</div>

<div id="ref-vaneph16">

\[88\] A. Vaneph, E. McNeil, and F. Rigaud, “An automated source
separation technology and its practical applications,” in *140th aes
convention*, 2016.

</div>

<div id="ref-leglaive15">

\[89\] S. Leglaive, R. Hennequin, and R. Badeau, “Singing voice
detection with deep recurrent neural networks,” in *IEEE international
conference on acoustics, speech and signal processing*, 2015.

</div>

<div id="ref-lee99">

\[90\] D. D. Lee and H. S. Seung, “Learning the parts of objects by
non-negative matrix factorization,” *Nature*, vol. 401, pp. 788–791,
Oct. 1999.

</div>

<div id="ref-lee01">

\[91\] D. D. Lee and H. S. Seung, “Algorithms for non-negative matrix
factorization,” in *Advances in neural information processing systems
13*, MIT Press, 2001, pp. 556–562.

</div>

<div id="ref-smaragdis03">

\[92\] P. Smaragdis and J. C. Brown, “Non-negative matrix factorization
for polyphonic music transcription,” in *IEEE workshop on applications
of signal processing to audio and acoustics*, 2003.

</div>

<div id="ref-virtanen07">

\[93\] T. Virtanen, “Monaural sound source separation by nonnegative
matrix factorization with temporal continuity and sparseness criteria,”
*IEEE Transactions on Audio, Speech, and Language Processing*, vol. 15,
no. 3, pp. 1066–1074, Mar. 2007.

</div>

<div id="ref-fevotte09">

\[94\] C. Févotte, “Nonnegative matrix factorization with the
Itakura-Saito divergence: With application to music analysis,” *Neural
Computation*, vol. 21, no. 3, pp. 793–830, Mar. 2009.

</div>

<div id="ref-common94">

\[95\] P. Common, “Independent component analysis, a new concept?”
*Signal Processing*, vol. 36, no. 3, pp. 287–314, Apr. 1994.

</div>

<div id="ref-vembu05">

\[96\] S. Vembu and S. Baumann, “Separation of vocals from polyphonic
audio recordings,” in *6th international conference on music information
retrieval*, 2005.

</div>

<div id="ref-hermansky90">

\[97\] H. Hermansky, “Perceptual linear predictive (PLP) analysis of
speech,” *Journal of the Acoustical Society of America*, vol. 87, no. 4,
pp. 1738–1752, Apr. 1990.

</div>

<div id="ref-nwe04">

\[98\] T. L. Nwe and Y. Wang, “Automatic detection of vocal segments in
popular songs,” in *5th international conference for music information
retrieval*, 2004.

</div>

<div id="ref-casey00">

\[99\] M. A. Casey and A. Westner, “Separation of mixed audio sources by
independent subspace analysis,” in *International computer music
conference*, 2000.

</div>

<div id="ref-chanrungutai08">

\[100\] A. Chanrungutai and C. A. Ratanamahatana, “Singing voice
separation for mono-channel music using non-negative matrix
factorization,” in *International conference on advanced technologies
for communications*, 2008.

</div>

<div id="ref-chanrungutai082">

\[101\] A. Chanrungutai and C. A. Ratanamahatana, “Singing voice
separation in mono-channel music,” in *International symposium on
communications and information technologies*, 2008.

</div>

<div id="ref-tikhonov63">

\[102\] A. N. Tikhonov, “Solution of incorrectly formulated problems and
the regularization method,” *Soviet Mathematics*, vol. 4, pp. 1035–1038,
1963.

</div>

<div id="ref-marxer122">

\[103\] R. Marxer and J. Janer, “A Tikhonov regularization method for
spectrum decomposition in low latency audio source separation,” in *IEEE
international conference on acoustics, speech and signal processing*,
2012.

</div>

<div id="ref-yang14">

\[104\] P.-K. Yang, C.-C. Hsu, and J.-T. Chien, “Bayesian singing-voice
separation,” in *15th international society for music information
retrieval conference*, 2014.

</div>

<div id="ref-chien15">

\[105\] J.-T. Chien and P.-K. Yang, “Bayesian factorization and learning
for monaural source separation,” *IEEE/ACM Transactions on Audio,
Speech, and Language Processing*, vol. 24, no. 1, pp. 185–195, Jan.
2015.

</div>

<div id="ref-cemgil09">

\[106\] A. T. Cemgil, “Bayesian inference for nonnegative matrix
factorisation models,” *Computational Intelligence and Neuroscience*,
vol. 2009, no. 4, pp. 1–17, Jan. 2009.

</div>

<div id="ref-schmidt09">

\[107\] M. N. Schmidt, O. Winther, and L. K. Hansen, “Bayesian
non-negative matrix factorization,” in *8th international conference on
independent component analysis and signal separation*, 2009.

</div>

<div id="ref-spiertz09">

\[108\] M. Spiertz and V. Gnann, “Source-filter based clustering for
monaural blind source separation,” in *12th international conference on
digital audio effects*, 2009.

</div>

<div id="ref-smaragdis09">

\[109\] P. Smaragdis and G. J. Mysore, “Separation by ‘humming’:
User-guided sound extraction from monophonic mixtures,” in *IEEE
workshop on applications of signal processing to audio and acoustics*,
2009.

</div>

<div id="ref-smaragdis07">

\[110\] P. Smaragdis, B. Raj, and M. Shashanka, “Supervised and
semi-supervised separation of sounds from single-channel mixtures,” in
*7th international conference on independent component analysis and
signal separation*, 2007.

</div>

<div id="ref-nakamuray15">

\[111\] T. Nakamuray and H. Kameoka, “\(L_p\)-norm non-negative matrix
factorization and its application to singing voice enhancement,” in
*IEEE international conference on acoustics, speech and signal
processing*, 2015.

</div>

<div id="ref-ortega70">

\[112\] J. M. Ortega and W. C. Rheinboldt, *Iterative solution of
nonlinear equations in several variables*. Academic Press, 1970.

</div>

<div id="ref-kameoka06">

\[113\] H. Kameoka, M. Goto, and S. Sagayama, “Selective amplifier of
periodic and non-periodic components in concurrent audio signals with
spectral control envelopes,” Information Processing Society of Japan,
2006.

</div>

<div id="ref-candes11">

\[114\] E. J. Candès, X. Li, Y. Ma, and J. Wright, “Robust principal
component analysis?” *Journal of the ACM*, vol. 58, no. 3, pp. 1–37, May
2011.

</div>

<div id="ref-huang12">

\[115\] P.-S. Huang, S. D. Chen, P. Smaragdis, and M. Hasegawa-Johnson,
“Singing-voice separation from monaural recordings using robust
principal component analysis,” in *IEEE international conference on
acoustics, speech and signal processing*, 2012.

</div>

<div id="ref-sprechmann12">

\[116\] P. Sprechmann, A. Bronstein, and G. Sapiro, “Real-time online
singing voice separation from monaural recordings using robust low-rank
modeling,” in *13th international society for music information
retrieval conference*, 2012.

</div>

<div id="ref-recht10">

\[117\] B. Recht, M. Fazel, and P. A. Parrilo, “Guaranteed minimum-rank
solutions of linear matrix equations via nuclear norm minimization,”
*SIAM Review*, vol. 52, no. 3, pp. 471–501, Aug. 2010.

</div>

<div id="ref-recht13">

\[118\] B. Recht and C. Ré, “Parallel stochastic gradient algorithms for
large-scale matrix completion,” *Mathematical Programming Computation*,
vol. 5, no. 2, pp. 201–226, Jun. 2013.

</div>

<div id="ref-gregor10">

\[119\] K. Gregor and Y. LeCun, “Learning fast approximations of sparse
coding,” in *27th international conference on machine learning*, 2010.

</div>

<div id="ref-zhang11">

\[120\] L. Zhang, Z. Chen, M. Zheng, and X. He, “Robust non-negative
matrix factorization,” *Frontiers of Electrical Electronic Engineering
China*, vol. 6, no. 2, pp. 192–200, Jun. 2011.

</div>

<div id="ref-jeong14">

\[121\] I.-Y. Jeong and K. Lee, “Vocal separation using extended robust
principal component analysis with Schatten \(P\)/\(L_p\)-norm and scale
compression,” in *International workshop on machine learning for signal
processing*, 2014.

</div>

<div id="ref-nie152">

\[122\] F. Nie, H. Wang, and H. Huang, “Joint Schatten \(p\)-norm and
\(l_p\)-norm robust matrix completion for missing value recovery,”
*Knowledge and Information Systems*, vol. 42, no. 3, pp. 525–544, Mar.
2015.

</div>

<div id="ref-yang13">

\[123\] Y.-H. Yang, “Low-rank representation of both singing voice and
music accompaniment via learned dictionaries,” in *14th international
society for music information retrieval conference*, 2013.

</div>

<div id="ref-mairal09">

\[124\] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online dictionary
learning for sparse coding,” in *26th annual international conference on
machine learning*, 2009.

</div>

<div id="ref-chan16">

\[125\] T.-S. T. Chan and Y.-H. Yang, “Complex and quaternionic
principal component pursuit and its application to audio separation,”
*IEEE Signal Processing Letters*, vol. 23, no. 2, pp. 287–291, Feb.
2016.

</div>

<div id="ref-peeters03">

\[126\] G. Peeters, “Deriving musical structures from signal analysis
for music audio summary generation: "Sequence" and "state" approach,” in
*International symposium on computer music multidisciplinary research*,
2003.

</div>

<div id="ref-dannenberg08">

\[127\] R. B. Dannenberg and M. Goto, “Music structure analysis from
acoustic signals,” in *Handbook of signal processing in acoustics*,
Springer New York, 2008, pp. 305–331.

</div>

<div id="ref-paulus10">

\[128\] J. Paulus, M. Müller, and A. Klapuri, “Audio-based music
structure analysis,” in *11th international society for music
information retrieval conference*, 2010.

</div>

<div id="ref-rafii11">

\[129\] Z. Rafii and B. Pardo, “A simple music/voice separation system
based on the extraction of the repeating musical structure,” in *IEEE
international conference on acoustics, speech and signal processing*,
2011.

</div>

<div id="ref-rafii13">

\[130\] Z. Rafii and B. Pardo, “REpeating Pattern Extraction Technique
(REPET): A simple method for music/voice separation,” *IEEE Transactions
on Audio, Speech, and Language Processing*, vol. 21, no. 1, pp. 73–84,
Jan. 2013.

</div>

<div id="ref-rafii14">

\[131\] Z. Rafii, A. Liutkus, and B. Pardo, “REPET for
background/foreground separation in audio,” in *Blind source
separation*, Springer Berlin Heidelberg, 2014, pp. 395–411.

</div>

<div id="ref-foote01">

\[132\] J. Foote and S. Uchihashi, “The beat spectrum: A new approach to
rhythm analysis,” in *IEEE international conference on multimedia and
expo*, 2001.

</div>

<div id="ref-seetharaman17">

\[133\] P. Seetharaman, F. Pishdadian, and B. Pardo, “Music/voice
separation using the 2D Fourier transform,” in *IEEE workshop on
applications of signal processing to audio and acoustics*, 2017.

</div>

<div id="ref-liutkus12">

\[134\] A. Liutkus, Z. Rafii, R. Badeau, B. Pardo, and G. Richard,
“Adaptive filtering for music/voice separation exploiting the
repeating musical structure,” in *IEEE international conference on
acoustics, speech and signal processing*, 2012.

</div>

<div id="ref-rafii12">

\[135\] Z. Rafii and B. Pardo, “Music/voice separation using the
similarity matrix,” in *13th international society for music information
retrieval conference*, 2012.

</div>

<div id="ref-foote99">

\[136\] J. Foote, “Visualizing music and audio using self-similarity,”
in *7th acm international conference on multimedia*, 1999.

</div>

<div id="ref-rafii133">

\[137\] Z. Rafii and B. Pardo, “Online REPET-SIM for real-time speech
enhancement,” in *IEEE international conference on acoustics, speech and
signal processing*, 2013.

</div>

<div id="ref-rafii15">

\[138\] Z. Rafii, A. Liutkus, and B. Pardo, “A simple user interface
system for recovering patterns repeating in time and frequency in
mixtures of sounds,” in *IEEE international conference on acoustics,
speech and signal processing*, 2015.

</div>

<div id="ref-fitzgerald12">

\[139\] D. FitzGerald, “Vocal separation using nearest neighbours and
median filtering,” in *23rd iet irish signals and systems conference*,
2012.

</div>

<div id="ref-liutkus14">

\[140\] A. Liutkus, Z. Rafii, B. Pardo, D. FitzGerald, and L. Daudet,
“Kernel spectrogram models for source separation,” in *4th joint
workshop on hands-free speech communication microphone arrays*, 2014.

</div>

<div id="ref-liutkus142">

\[141\] A. Liutkus, D. FitzGerald, Z. Rafii, B. Pardo, and L. Daudet,
“Kernel additive models for source separation,” *IEEE Transactions on
Signal Processing*, vol. 62, no. 16, pp. 4298–4310, Aug. 2014.

</div>

<div id="ref-liutkus15">

\[142\] A. Liutkus, D. FitzGerald, and Z. Rafii, “Scalable audio
separation with light kernel additive modelling,” in *IEEE international
conference on acoustics, speech and signal processing*, 2015.

</div>

<div id="ref-pratzlich15">

\[143\] T. Prätzlich, R. Bittner, A. Liutkus, and M. Müller, “Kernel
additive modeling for interference reduction in multi-channel music
recordings,” in *IEEE international conference on acoustics, speech and
signal processing*, 2015.

</div>

<div id="ref-fanoyela17">

\[144\] D. F. Yela, S. Ewert, D. FitzGerald, and M. Sandler,
“Interference reduction in music recordings combining kernel additive
modelling and non-negative matrix factorization,” in *IEEE international
conference on acoustics, speech and signal processing*, 2017.

</div>

<div id="ref-moussallam12">

\[145\] M. Moussallam, G. Richard, and L. Daudet, “Audio source
separation informed by redundancy with greedy multiscale
decompositions,” in *20th european signal processing conference*,
2012.

</div>

<div id="ref-mallat93">

\[146\] S. G. Mallat and Z. Zhang, “Matching pursuits with
time-frequency dictionaries,” *IEEE Transactions on Signal Processing*,
vol. 41, no. 12, pp. 3397–3415, Dec. 1993.

</div>

<div id="ref-deif152">

\[147\] H. Deif, D. FitzGerald, W. Wang, and L. Gan, “Separation of
vocals from monaural music recordings using diagonal median filters and
practical time-frequency parameters,” in *IEEE international symposium
on signal processing and information technology*, 2015.

</div>

<div id="ref-fitzgerald102">

\[148\] D. FitzGerald and M. Gainza, “Single channel vocal separation
using median filtering and factorisation techniques,” *ISAST
Transactions on Electronic and Signal Processing*, vol. 4, no. 1, pp.
62–73, Jan. 2010.

</div>

<div id="ref-lee152">

\[149\] J.-Y. Lee and H.-G. Kim, “Music and voice separation using
log-spectral amplitude estimator based on kernel spectrogram models
backfitting,” *Journal of the Acoustical Society of Korea*, vol. 34, no.
3, pp. 227–233, 2015.

</div>

<div id="ref-lee15">

\[150\] J.-Y. Lee, H.-S. Cho, and H.-G. Kim, “Vocal separation from
monaural music using adaptive auditory filtering based on kernel
back-fitting,” in *Interspeech*, 2015.

</div>

<div id="ref-cho15">

\[151\] H.-S. Cho, J.-Y. Lee, and H.-G. Kim, “Singing voice separation
from monaural music based on kernel back-fitting using beta-order
spectral amplitude estimation,” in *16th international society for music
information retrieval conference*, 2015.

</div>

<div id="ref-kim16">

\[152\] H.-G. Kim and J. Y. Kim, “Music/voice separation based on kernel
back-fitting using weighted \(\beta\)-order MMSE estimation,” *ETRI
Journal*, vol. 38, no. 3, pp. 510–517, Jun. 2016.

</div>

<div id="ref-plourde08">

\[153\] E. Plourde and B. Champagne, “Auditory-based spectral amplitude
estimators for speech enhancement,” *IEEE Transactions on Audio, Speech,
and Language Processing*, vol. 16, no. 8, pp. 1614–1623, Nov. 2008.

</div>

<div id="ref-raj07">

\[154\] B. Raj, P. Smaragdis, M. Shashanka, and R. Singh, “Separating a
foreground singer from background music,” in *International symposium on
frontiers of research on speech and music*, 2007.

</div>

<div id="ref-smaragdis06">

\[155\] P. Smaragdis and B. Raj, “Shift-invariant probabilistic latent
component analysis,” MERL, 2006.

</div>

<div id="ref-raj05">

\[156\] B. Raj and P. Smaragdis, “Latent variable decomposition of
spectrograms for single channel speaker separation,” in *IEEE workshop
on applications of signal processing to audio and acoustics*, 2005.

</div>

<div id="ref-han11">

\[157\] J. Han and C.-W. Chen, “Improving melody extraction using
probabilistic latent component analysis,” in *IEEE international
conference on acoustics, speech and signal processing*, 2011.

</div>

<div id="ref-boersma93">

\[158\] P. Boersma, “Accurate short-term analysis of the fundamental
frequency and the harmonics-to-noise ratio of a sampled sound,” in *IFA
proceedings 17*, 1993.

</div>

<div id="ref-gomez12">

\[159\] E. Gómez, F. J. C. Quesada, J. Salamon, J. Bonada, P. V. Candea,
and P. C. Molero, “Predominant fundamental frequency estimation vs
singing voice separation for the automatic transcription of accompanied
flamenco singing,” in *13th international society for music information
retrieval conference*, 2012.

</div>

<div id="ref-ono08">

\[160\] N. Ono, K. Miyamoto, J. L. Roux, H. Kameoka, and S. Sagayama,
“Separation of a monaural audio signal into harmonic/percussive
components by complementary diffusion on spectrogram,” in *16th european
signal processing conference*, 2008.

</div>

<div id="ref-papadopoulos14">

\[161\] H. Papadopoulos and D. P. Ellis, “Music-content-adaptive robust
principal component analysis for a semantically consistent separation of
foreground and background in music audio signals,” in *17th
international conference on digital audio effects*, 2014.

</div>

<div id="ref-chan15">

\[162\] T.-S. Chan *et al.*, “Vocal activity informed singing voice
separation with the iKala dataset,” in *IEEE international conference on
acoustics, speech and signal processing*, 2015.

</div>

<div id="ref-jeong17">

\[163\] I.-Y. Jeong and K. Lee, “Singing voice separation using RPCA
with weighted \(l_1\)-norm,” in *13th international conference on latent
variable analysis and signal separation*, 2017.

</div>

<div id="ref-virtanen08">

\[164\] T. Virtanen, A. Mesaros, and M. Ryynänen, “Combining pitch-based
inference and non-negative spectrogram factorization in separating
vocals from polyphonic music,” in *ISCA tutorial and research workshop
on statistical and perceptual audition*, 2008.

</div>

<div id="ref-wang11">

\[165\] Y. Wang and Z. Ou, “Combining HMM-based melody extraction and
NMF-based soft masking for separating voice and accompaniment from
monaural audio,” in *IEEE international conference on acoustics, speech
and signal processing*, 2011.

</div>

<div id="ref-klapuri06">

\[166\] A. Klapuri, “Multiple fundamental frequency estimation by
summing harmonic amplitudes,” in *7th international conference on music
information retrieval*, 2006.

</div>

<div id="ref-hsu09">

\[167\] C.-L. Hsu, L.-Y. Chen, J.-S. R. Jang, and H.-J. Li, “Singing
pitch extraction from monaural polyphonic songs by contextual audio
modeling and singing harmonic enhancement,” in *10th international
society for music information retrieval conference*, 2009.

</div>

<div id="ref-rafii142">

\[168\] Z. Rafii, Z. Duan, and B. Pardo, “Combining rhythm-based and
pitch-based methods for background and melody separation,” *IEEE/ACM
Transactions on Audio, Speech, and Language Processing*, vol. 22, no.
12, pp. 1884–1893, Sep. 2014.

</div>

<div id="ref-duan10">

\[169\] Z. Duan and B. Pardo, “Multiple fundamental frequency estimation
by modeling spectral peaks and non-peak regions,” *IEEE Transactions on
Audio, Speech, and Language Processing*, vol. 18, no. 8, pp. 2121–2133,
Nov. 2010.

</div>

<div id="ref-venkataramani14">

\[170\] S. Venkataramani, N. Nayak, P. Rao, and R. Velmurugan, “Vocal
separation using singer-vowel priors obtained from polyphonic audio,” in
*15th international society for music information retrieval conference*,
2014.

</div>

<div id="ref-rao10">

\[171\] V. Rao and P. Rao, “Vocal melody extraction in the presence of
pitched accompaniment in polyphonic music,” *IEEE Transactions on Audio,
Speech, and Language Processing*, vol. 18, no. 8, pp. 2145–2154, Nov.
2010.

</div>

<div id="ref-rao11">

\[172\] V. Rao, C. Gupta, and P. Rao, “Context-aware features for
singing voice detection in polyphonic music,” in *International workshop
on adaptive multimedia retrieval*, 2011.

</div>

<div id="ref-kim112">

\[173\] M. Kim, J. Yoo, K. Kang, and S. Choi, “Nonnegative matrix
partial co-factorization for spectral and temporal drum source
separation,” *IEEE Journal of Selected Topics in Signal Processing*,
vol. 5, no. 6, pp. 1192–1204, Oct. 2011.

</div>

<div id="ref-zhou14">

\[174\] L. Zhang, Z. Chen, M. Zheng, and X. He, “Nonnegative matrix and
tensor factorizations: An algorithmic perspective,” *IEEE Signal
Processing Magazine*, vol. 31, no. 3, pp. 54–65, May 2014.

</div>

<div id="ref-ikemiya15">

\[175\] Y. Ikemiya, K. Yoshii, and K. Itoyama, “Singing voice analysis
and editing based on mutually dependent F0 estimation and source
separation,” in *IEEE international conference on acoustics, speech and
signal processing*, 2015.

</div>

<div id="ref-ikemiya16">

\[176\] Y. Ikemiya, K. Itoyama, and K. Yoshii, “Singing voice separation
and vocal F0 estimation based on mutual combination of robust principal
component analysis and subharmonic summation,” *IEEE/ACM Transactions on
Audio, Speech, and Language Processing*, vol. 24, no. 11, pp. 2084–2095,
Nov. 2016.

</div>

<div id="ref-hermes88">

\[177\] D. J. Hermes, “Measurement of pitch by subharmonic summation,”
*Journal of the Acoustical Society of America*, vol. 83, no. 1, pp.
257–264, Jan. 1988.

</div>

<div id="ref-dobashi15">

\[178\] A. Dobashi, Y. Ikemiya, K. Itoyama, and K. Yoshii, “A music
performance assistance system based on vocal, harmonic, and percussive
source separation and content visualization for music audio signals,” in
*12th sound and music computing conference*, 2015.

</div>

<div id="ref-hu15">

\[179\] Y. Hu and G. Liu, “Separation of singing voice using nonnegative
matrix partial co-factorization for singer identification,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 23, no. 4,
pp. 643–653, Apr. 2015.

</div>

<div id="ref-yoo10">

\[180\] J. Yoo, M. Kim, K. Kang, and S. Choi, “Nonnegative matrix
partial co-factorization for drum source separation,” in *IEEE
international conference on acoustics, speech and signal processing*,
2010.

</div>

<div id="ref-boersma01">

\[181\] P. Boersma, “PRAAT, a system for doing phonetics by computer,”
*Glot International*, vol. 5, no. 9/10, pp. 341–347, Dec. 2001.

</div>

<div id="ref-li09">

\[182\] Y. Li, J. Woodruff, and D. Wang, “Monaural musical sound
separation based on pitch and common amplitude modulation,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 17, no. 7,
pp. 1361–1371, Sep. 2009.

</div>

<div id="ref-raj04">

\[183\] B. Raj, M. L. Seltzer, and R. M. Stern, “Reconstruction of
missing features for robust speech recognition,” *Speech Communication*,
vol. 43, no. 4, pp. 275–296, Sep. 2004.

</div>

<div id="ref-hu16">

\[184\] Y. Hu and G. Liu, “Monaural singing voice separation by
non-negative matrix partial co-factorization with temporal continuity
and sparsity criteria,” in *12th international conference on intelligent
computing*, 2016.

</div>

<div id="ref-zhang15">

\[185\] X. Zhang, W. Li, and B. Zhu, “Latent time-frequency component
analysis: A novel pitch-based approach for singing voice separation,” in
*IEEE international conference on acoustics, speech and signal
processing*, 2015.

</div>

<div id="ref-decheveigne02">

\[186\] A. de Cheveigné and H. Kawahara, “YIN, a fundamental frequency
estimator for speech and music,” *Journal of the Acoustical Society of
America*, vol. 111, no. 4, pp. 1917–1930, Apr. 2002.

</div>

<div id="ref-zhu15">

\[187\] B. Zhu, W. Li, and L. Li, “Towards solving the bottleneck of
pitch-based singing voice separation,” in *23rd acm international
conference on multimedia*, 2015.

</div>

<div id="ref-durrieu08">

\[188\] J.-L. Durrieu, G. Richard, and B. David, “Singer melody
extraction in polyphonic signals using source separation methods,” in
*IEEE international conference on acoustics, speech and signal
processing*, 2008.

</div>

<div id="ref-durrieu09">

\[189\] J.-L. Durrieu, G. Richard, and B. David, “An iterative approach
to monaural musical mixture de-soloing,” in *IEEE international
conference on acoustics, speech and signal processing*, 2009.

</div>

<div id="ref-durrieu10">

\[190\] J.-L. Durrieu, G. Richard, B. David, and C. Févotte,
“Source/filter model for unsupervised main melody extraction from
polyphonic audio signals,” *IEEE Transactions on Audio, Speech, and
Language Processing*, vol. 18, no. 3, pp. 564–575, Mar. 2010.

</div>

<div id="ref-ozerov07">

\[191\] A. Ozerov, P. Philippe, F. Bimbot, and R. Gribonval, “Adaptation
of Bayesian models for single-channel source separation and its
application to voice/music separation in popular songs,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 15, no. 5,
pp. 1564–1578, Jul. 2007.

</div>

<div id="ref-klatt90">

\[192\] D. H. Klatt and L. C. Klatt, “Analysis, synthesis, and
perception of voice quality variations among female and male talkers,”
*Journal of the Acoustical Society of America*, vol. 87, no. 2, pp.
820–857, Feb. 1990.

</div>

<div id="ref-benaroya032">

\[193\] L. Benaroya, L. Mcdonagh, F. Bimbot, and R. Gribonval, “Non
negative sparse representation for Wiener based source separation with a
single sensor,” in *IEEE international conference on acoustics, speech
and signal processing*, 2003.

</div>

<div id="ref-dhillon05">

\[194\] I. S. Dhillon and S. Sra, “Generalized nonnegative matrix
approximations with Bregman divergences,” in *Advances in neural
information processing systems 18*, MIT Press, 2005, pp. 283–290.

</div>

<div id="ref-benaroya06">

\[195\] L. Benaroya, F. Bimbot, and R. Gribonval, “Audio source
separation with a single sensor,” *IEEE Transactions on Audio, Speech,
and Language Processing*, vol. 14, no. 1, pp. 191–199, Jan. 2006.

</div>

<div id="ref-durrieu12">

\[196\] J.-L. Durrieu and J.-P. Thiran, “Musical audio source separation
based on user-selected F0 track,” in *10th international conference on
latent variable analysis and signal separation*, 2012.

</div>

<div id="ref-fuentes2012">

\[197\] B. Fuentes, R. Badeau, and G. Richard, “Blind harmonic adaptive
decomposition applied to supervised source separation,” in *Signal
processing conference (eusipco), 2012 proceedings of the 20th european*,
2012, pp. 2654–2658.

</div>

<div id="ref-brown91">

\[198\] J. C. Brown, “Calculation of a constant Q spectral transform,”
*Journal of the Acoustical Society of America*, vol. 89, no. 1, pp.
425–434, Jan. 1991.

</div>

<div id="ref-brown92">

\[199\] J. C. Brown and M. S. Puckette, “An efficient algorithm for the
calculation of a constant Q transform,” *Journal of the Acoustical
Society of America*, vol. 92, no. 5, pp. 2698–2701, Nov. 1992.

</div>

<div id="ref-schorkhuber10">

\[200\] C. Schörkhuber and A. Klapuri, “Constant-Q transform toolbox,”
in *7th sound and music computing conference*, 2010.

</div>

<div id="ref-durrieu11">

\[201\] J.-L. Durrieu, B. David, and G. Richard, “A musically motivated
mid-level representation for pitch estimation and musical audio source
separation,” *IEEE Journal of Selected Topics in Signal Processing*,
vol. 5, no. 6, pp. 1180–1191, Oct. 2011.

</div>

<div id="ref-joder12">

\[202\] C. Joder and B. Schuller, “Score-informed leading voice
separation from monaural audio,” in *13th international society for
music information retrieval conference*, 2012.

</div>

<div id="ref-joder11">

\[203\] C. Joder, S. Essid, and G. Richard, “A conditional random field
framework for robust and scalable audio-to-score matching,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 19, no. 8,
pp. 2385–2397, Nov. 2011.

</div>

<div id="ref-zhao14">

\[204\] R. Zhao, S. Lee, D.-Y. Huang, and M. Dong, “Soft constrained
leading voice separation with music score guidance,” in *9th
international symposium on chinese spoken language*, 2014.

</div>

<div id="ref-durrieu092">

\[205\] J.-L. Durrieu, A. Ozerov, C. Févotte, G. Richard, and B. David,
“Main instrument separation from stereophonic audio signals using a
source/filter model,” in *17th european signal processing conference*,
2009.

</div>

<div id="ref-janer13">

\[206\] J. Janer and R. Marxer, “Separation of unvoiced fricatives in
singing voice mixtures with semi-supervised NMF,” in *16th international
conference on digital audio effects*, 2013.

</div>

<div id="ref-janer12">

\[207\] J. Janer, R. Marxer, and K. Arimoto, “Combining a harmonic-based
NMF decomposition with transient analysis for instantaneous percussion
separation,” in *IEEE international conference on acoustics, speech and
signal processing*, 2012.

</div>

<div id="ref-marxer13">

\[208\] R. Marxer and J. Janer, “Modelling and separation of singing
voice breathiness in polyphonic mixtures,” in *16th international
conference on digital audio effects*, 2013.

</div>

<div id="ref-degottex11">

\[209\] G. Degottex, A. Roebel, and X. Rodet, “Pitch transposition and
breathiness modification using a glottal source model and its adapted
vocal-tract filter,” in *IEEE international conference on acoustics,
speech and signal processing*, 2011.

</div>

<div id="ref-ozerov102">

\[210\] A. Ozerov, E. Vincent, and F. Bimbot, “A general modular
framework for audio source separation,” in *9th international conference
on latent variable analysis and signal separation*, 2010.

</div>

<div id="ref-ozerov12">

\[211\] A. Ozerov, E. Vincent, and F. Bimbot, “A general flexible
framework for the handling of prior information in audio source
separation,” *IEEE Transactions on Audio, Speech, and Language
Processing*, vol. 20, no. 4, pp. 1118–1133, May 2012.

</div>

<div id="ref-salaun14">

\[212\] Y. Salaün *et al.*, “The flexible audio source separation
toolbox version 2.0,” in *IEEE international conference on acoustics,
speech and signal processing*, 2014.

</div>

<div id="ref-hennequin16">

\[213\] R. Hennequin and F. Rigaud, “Long-term reverberation modeling
for under-determined audio source separation with application to vocal
melody extraction,” in *17th international society for music information
retrieval conference*, 2016.

</div>

<div id="ref-singh10">

\[214\] R. Singh, B. Raj, and P. Smaragdis, “Latent-variable
decomposition based dereverberation of monaural and multi-channel
signals,” in *IEEE international conference on acoustics, speech and
signal processing*, 2010.

</div>

<div id="ref-ono082">

\[215\] N. Ono, K. Miyamoto, H. Kameoka, and S. Sagayama, “A real-time
equalizer of harmonic and percussive components in music signals,” in
*9th international conference on music information retrieval*, 2008.

</div>

<div id="ref-fitzgerald10">

\[216\] D. FitzGerald, “Harmonic/percussive separation using median
filtering,” in *13th international conference on digital audio effects*,
2010.

</div>

<div id="ref-yang12">

\[217\] Y.-H. Yang, “On sparse and low-rank matrix decomposition for
singing voice separation,” in *20th acm international conference on
multimedia*, 2012.

</div>

<div id="ref-jeong142">

\[218\] I.-Y. Jeong and K. Lee, “Vocal separation from monaural music
using temporal/spectral continuity and sparsity constraints,” *IEEE
Signal Processing Letters*, vol. 21, no. 10, pp. 1197–1200, Jun. 2014.

</div>

<div id="ref-ochiai15">

\[219\] E. Ochiai, T. Fujisawa, and M. Ikehara, “Vocal separation by
constrained non-negative matrix factorization,” in *Asia-pacific signal
and information processing association annual summit and conference*,
2015.

</div>

<div id="ref-watanabe16">

\[220\] T. Watanabe, T. Fujisawa, and M. Ikehara, “Vocal separation
using improved robust principal component analysis and post-processing,”
in *IEEE 59th international midwest symposium on circuits and systems*,
2016.

</div>

<div id="ref-raguet13">

\[221\] H. Raguet, J. Fadili, and and Gabriel Peyré, “A generalized
forward-backward splitting,” *SIAM Journal on Imaging Sciences*, vol. 6,
no. 3, pp. 1199–1226, Jul. 2013.

</div>

<div id="ref-hayashi16">

\[222\] A. Hayashi, H. Kameoka, T. Matsubayashi, and H. Sawada,
“Non-negative periodic component analysis for music source
separation,” in *Asia-pacific signal and information processing
association annual summit and conference*, 2016.

</div>

<div id="ref-fitzgerald09">

\[223\] D. FitzGerald, M. Cranitch, and E. Coyle, “Using tensor
factorisation models to separate drums from polyphonic music,” in *12th
international conference on digital audio effects*, 2009.

</div>

<div id="ref-tachibana14">

\[224\] H. Tachibana, N. Ono, and S. Sagayama, “Singing voice
enhancement in monaural music signals based on two-stage
harmonic/percussive sound separation on multiple resolution
spectrograms,” *IEEE/ACM Transactions on Audio, Speech and Language
Processing*, vol. 22, no. 1, pp. 228–237, Jan. 2014.

</div>

<div id="ref-tachibana10">

\[225\] H. Tachibana, T. Ono, N. Ono, and S. Sagayama, “Melody line
estimation in homophonic music audio signals based on
temporal-variability of melodic source,” in *IEEE international
conference on acoustics, speech and signal processing*, 2010.

</div>

<div id="ref-tachibana16">

\[226\] H. Tachibana, N. Ono, and S. Sagayama, “A real-time
audio-to-audio karaoke generation system for monaural recordings based
on singing voice suppression and key conversion techniques,” *Journal of
Information Processing*, vol. 24, no. 3, pp. 470–482, May 2016.

</div>

<div id="ref-ono10">

\[227\] N. Ono *et al.*, “Harmonic and percussive sound separation and
its application to MIR-related tasks,” in *Advances in music information
retrieval*, Springer Berlin Heidelberg, 2010, pp. 213–236.

</div>

<div id="ref-tachibana12">

\[228\] H. Tachibana, H. Kameoka, N. Ono, and S. Sagayama, “Comparative
evaluations of multiple harmonic/percussive sound separation techniques
based on anisotropic smoothness of spectrogram,” in *IEEE international
conference on acoustics, speech and signal processing*, 2012.

</div>

<div id="ref-deif15">

\[229\] H. Deif, W. Wang, L. Gan, and S. Alhashmi, “A local
discontinuity based approach for monaural singing voice separation from
accompanying music with multi-stage non-negative matrix factorization,”
in *IEEE global conference on signal and information processing*, 2015.

</div>

<div id="ref-zhu13">

\[230\] B. Zhu, W. Li, R. Li, and X. Xue, “Multi-stage non-negative
matrix factorization for monaural singing voice separation,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 21, no.
10, pp. 2096–2107, Oct. 2013.

</div>

<div id="ref-driedger15">

\[231\] J. Driedger and M. Müller, “Extracting singing voice from music
recordings by cascading audio decomposition techniques,” in *IEEE
international conference on acoustics, speech and signal processing*,
2015.

</div>

<div id="ref-driedger14">

\[232\] J. Driedger, M. Müller, and S. Disch, “Extending
harmonic-percussive separation of audio signals,” in *15th international
society for music information retrieval conference*, 2014.

</div>

<div id="ref-talmon11">

\[233\] R. Talmon, I. Cohen, and S. Gannot, “Transient noise reduction
using nonlocal diffusion filters,” *IEEE/ACM Transactions on Audio,
Speech and Language Processing*, vol. 19, no. 6, pp. 1584–1599, Aug.
2011.

</div>

<div id="ref-hsu12">

\[234\] C.-L. Hsu, D. Wang, J.-S. R. Jang, and K. Hu, “A tandem
algorithm for singing pitch extraction and voice separation from music
accompaniment,” *IEEE Transactions on Audio, Speech, and Language
Processing*, vol. 20, no. 5, pp. 1482–1491, Jul. 2012.

</div>

<div id="ref-hu10">

\[235\] G. Hu and D. Wang, “A tandem algorithm for pitch estimation and
voiced speech segregation,” *IEEE Transactions on Audio, Speech, and
Language Processing*, vol. 18, no. 8, pp. 2067–2079, Nov. 2010.

</div>

<div id="ref-rumelhart86">

\[236\] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning
internal representations by error propagation,” in *Parallel distributed
processing: Explorations in the microstructure of cognition, vol. 1*,
MIT Press Cambridge, 1986, pp. 318–362.

</div>

<div id="ref-bryan13">

\[237\] N. J. Bryan and G. J. Mysore, “Interactive user-feedback for
sound source separation,” in *International conference on intelligent
user-interfaces, workshop on interactive machine learning*, 2013.

</div>

<div id="ref-bryan132">

\[238\] N. J. Bryan and G. J. Mysore, “An efficient posterior
regularized latent variable model for interactive sound source
separation,” in *30th international conference on machine learning*,
2013.

</div>

<div id="ref-bryan133">

\[239\] N. J. Bryan and G. J. Mysore, “Interactive refinement of
supervised and semi-supervised sound source separation estimates,” in
*IEEE international conference on acoustics, speech, and signal
processing*, 2013.

</div>

<div id="ref-ganchev10">

\[240\] K. Ganchev, J. Graça, J. Gillenwater, and B. Taskar, “Posterior
regularization for structured latent variable models,” *Journal of
Machine Learning Research*, vol. 11, pp. 2001–2049, Mar. 2010.

</div>

<div id="ref-ozerov13">

\[241\] A. Ozerov, N. Duong, and L. Chevallier, “Weighted nonnegative
tensor factorization: On monotonicity of multiplicative update rules and
application to user-guided audio source separation,” Technicolor, 2013.

</div>

<div id="ref-jaureguiberry13">

\[242\] X. Jaureguiberry, G. Richard, P. Leveau, R. Hennequin, and E.
Vincent, “Introducing a simple fusion framework for audio source
separation,” in *IEEE international workshop on machine learning for
signal processing*, 2013.

</div>

<div id="ref-jaureguiberry14">

\[243\] X. Jaureguiberry, E. Vincent, and G. Richard, “Variational
Bayesian model averaging for audio source separation,” in *IEEE workshop
on statistical signal processing workshop*, 2014.

</div>

<div id="ref-jaureguiberry16">

\[244\] X. Jaureguiberry, E. Vincent, and G. Richard, “Fusion methods
for speech enhancement and audio source separation,” *IEEE/ACM
Transactions on Audio, Speech, and Language Processing*, vol. 24, no. 7,
pp. 1266–1279, Jul. 2016.

</div>

<div id="ref-hoeting99">

\[245\] J. A. Hoeting, D. Madigan, A. E. Raftery, and C. T. Volinsky,
“Bayesian model averaging: A tutorial,” *Statistical Science*, vol.
14, no. 4, pp. 382–417, Nov. 1999.

</div>

<div id="ref-mcvicar16">

\[246\] M. McVicar, R. Santos-Rodriguez, and T. D. Bie, “Learning to
separate vocals from polyphonic mixtures via ensemble methods and
structured output prediction,” in *IEEE international conference on
acoustics, speech and signal processing*, 2016.

</div>

<div id="ref-jain90">

\[247\] A. K. Jain and F. Farrokhnia, “Unsupervised texture segmentation
using Gabor filters,” in *IEEE international conference on systems, man
and cybernetics*, 1990.

</div>

<div id="ref-huang14">

\[248\] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,
“Singing-voice separation from monaural recordings using deep
recurrent neural networks,” in *15th international society for music
information retrieval conference*, 2014.

</div>

<div id="ref-lacoste-julien13">

\[249\] S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher,
“Block-coordinate Frank-Wolfe optimization for structural SVMs,” in
*30th international conference on machine learning*, 2013.

</div>

<div id="ref-manilow17">

\[250\] E. Manilow, P. Seetharaman, F. Pishdadian, and B. Pardo,
“Predicting algorithm efficacy for adaptive, multi-cue source
separation,” in *IEEE workshop on applications of signal processing to
audio and acoustics*, 2017.

</div>

<div id="ref-wolf14">

\[251\] G. Wolf, S. Mallat, and S. Shamma, “Audio source separation with
time-frequency velocities,” in *IEEE international workshop on machine
learning for signal processing*, 2014.

</div>

<div id="ref-wolf16">

\[252\] G. Wolf, S. Mallat, and S. Shamma, “Rigid motion model for audio
source separation,” *IEEE Transactions on Signal Processing*, vol. 64,
no. 7, pp. 1822–1831, Apr. 2016.

</div>

<div id="ref-anden14">

\[253\] J. Andén and S. Mallat, “Deep scattering spectrum,” *IEEE
Transactions on Signal Processing*, vol. 62, no. 16, pp. 4114–4128, Aug.
2014.

</div>

<div id="ref-bernard01">

\[254\] C. P. Bernard, “Discrete wavelet analysis for fast optic flow
computation,” *Applied and Computational Harmonic Analysis*, vol. 11,
no. 1, pp. 32–63, Jul. 2001.

</div>

<div id="ref-yen14">

\[255\] F. Yen, Y.-J. Luo, and T.-S. Chi, “Singing voice separation
using spectro-temporal modulation features,” in *15th international
society for music information retrieval conference*, 2014.

</div>

<div id="ref-yen15">

\[256\] F. Yen, M.-C. Huang, and T.-S. Chi, “A two-stage singing voice
separation algorithm using spectro-temporal modulation features,” in
*Interspeech*, 2015.

</div>

<div id="ref-chi05">

\[257\] T. Chi, P. Rub, and S. A. Shamma, “Multiresolution
spectrotemporal analysis of complex sounds,” *Journal of the Acoustical
Society of America*, vol. 118, no. 2, pp. 887–906, Aug. 2005.

</div>

<div id="ref-chi99">

\[258\] T. Chi, Y. Gao, M. C. Guyton, P. Ru, and S. Shamma,
“Spectro-temporal modulation transfer functions and speech
intelligibility,” *Journal of the Acoustical Society of America*, vol.
106, no. 5, pp. 2719–2732, Nov. 1999.

</div>

<div id="ref-chan17">

\[259\] T.-S. T. Chan and Y.-H. Yang, “Informed group-sparse
representation for singing voice separation,” *IEEE Signal Processing
Letters*, vol. 24, no. 2, pp. 156–160, Feb. 2017.

</div>

<div id="ref-yuan06">

\[260\] M. Yuan and Y. Lin, “Model selection and estimation in
regression with grouped variables,” *Journal of the Royal Statistical
Society Series B*, vol. 68, no. 1, pp. 49–67, Dec. 2006.

</div>

<div id="ref-ma16">

\[261\] S. Ma, “Alternating proximal gradient method for convex
minimization,” *Journal of Scientific Computing*, vol. 68, no. 2, pp.
546–572, Aug. 2016.

</div>

<div id="ref-liu13">

\[262\] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, “Robust
recovery of subspace structures by low-rank representation,” *IEEE
Transactions on Pattern Analysis and Machine Intelligence*, vol. 35, no.
1, pp. 171–184, Jan. 2007.

</div>

<div id="ref-varga93">

\[263\] A. Varga and H. J. Steeneken, “Assessment for automatic speech
recognition: II. NOISEX-92: A database and an experiment to study the
effect of additive noise on speech recognition systems,” *Speech
Communication*, vol. 12, no. 3, pp. 247–251, Jul. 1993.

</div>

<div id="ref-garofolo93">

\[264\] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, and D.
S. Pallett, “DARPA TIMIT acoustic-phonetic continuous speech corpus
CD-ROM. NIST speech disc 1-1.1,” *NASA STI/Recon technical report n*.
1993.

</div>

<div id="ref-sturmel12">

\[265\] N. Sturmel *et al.*, “Linear mixing models for active listening
of music productions in realistic studio conditions,” in *132nd aes
convention*, 2012.

</div>

<div id="ref-MTGMASSdb">

\[266\] M. Vinyes, “MTG MASS database.” 2008.

</div>

<div id="ref-vincent09">

\[267\] E. Vincent, S. Araki, and P. Bofill, “The 2008 signal separation
evaluation campaign: A community-based approach to large-scale
evaluation,” in *8th international conference on independent component
analysis and signal separation*, 2009.

</div>

<div id="ref-araki10">

\[268\] S. Araki *et al.*, “The 2010 signal separation evaluation
campaign (SiSEC2010): - audio source separation -,” in *9th
international conference on latent variable analysis and signal
separation*, 2010.

</div>

<div id="ref-araki12">

\[269\] S. Araki *et al.*, “The 2011 signal separation evaluation
campaign (SiSEC2011): - audio source separation -,” in *10th
international conference on latent variable analysis and signal
separation*, 2012.

</div>

<div id="ref-vincent12">

\[270\] E. Vincent *et al.*, “The signal separation evaluation campaign
(2007-2010): Achievements and remaining challenges,” *Signal
Processing*, vol. 92, no. 8, pp. 1928–1936, Aug. 2012.

</div>

<div id="ref-ono15">

\[271\] N. Ono, Z. Rafii, D. Kitamura, N. Ito, and A. Liutkus, “The 2015
signal separation evaluation campaign,” in *12th international
conference on latent variable analysis and signal separation*, 2015.

</div>

<div id="ref-liutkus17">

\[272\] A. Liutkus *et al.*, “The 2016 signal separation evaluation
campaign,” in *13th international conference on latent variable analysis
and signal separation*, 2017.

</div>

<div id="ref-liutkus11">

\[273\] A. Liutkus, R. Badeau, and G. Richard, “Gaussian processes for
underdetermined source separation,” *IEEE Transactions on Audio, Speech,
and Language Processing*, vol. 59, no. 7, pp. 3155–3167, Feb. 2011.

</div>

<div id="ref-bittner14">

\[274\] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam, and and
Juan P. Bello, “MedleyDB: A multitrack dataset for annotation-intensive
mir research,” in *15th international society for music information
retrieval conference*, 2014.

</div>

<div id="ref-rafii17">

\[275\] Z. Rafii, A. Liutkus, F.-R. Stöter, S. I. Mimilakis, and R.
Bittner, “MUSDB18, a dataset for audio source separation.” Dec-2017.

</div>

<div id="ref-ozerov05">

\[276\] A. Ozerov, P. Philippe, R. Gribonval, and F. Bimbot, “One
microphone singing voice separation using source-adapted models,” in
*IEEE workshop on applications of signal processing to audio and
acoustics*, 2005.

</div>

<div id="ref-tsai04">

\[277\] W.-H. Tsai, D. Rogers, and H.-M. Wang, “Blind clustering of
popular music recordings based on singer voice characteristics,”
*Computer Music Journal*, vol. 28, no. 3, pp. 68–78, 2004.

</div>

<div id="ref-gauvain94">

\[278\] J.-L. Gauvain and C.-H. Lee, “Maximum a posteriori estimation
for multivariate Gaussian mixture observations of Markov chains,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 2, no. 2,
pp. 291–298, Apr. 1994.

</div>

<div id="ref-vincent10">

\[279\] E. Vincent, M. Jafari, S. Abdallah, M. Plumbley, and M. Davies,
“Probabilistic modeling paradigms for audio source separation,” in
*Machine audition: Principles, algorithms and systems*, IGI Global,
2010, pp. 162–185.

</div>

<div id="ref-rafii132">

\[280\] Z. Rafii, D. L. Sun, F. G. Germain, and G. J. Mysore, “Combining
modeling of singing voice and background music for automatic separation
of musical mixtures,” in *14th international society for music
information retrieval conference*, 2013.

</div>

<div id="ref-boulanger-lewandowski14">

\[281\] N. Boulanger-Lewandowski, G. J. Mysore, and M. Hoffman,
“Exploiting long-term temporal dependencies in NMF using recurrent
neural networks with application to source separation,” in *IEEE
international conference on acoustics, speech and signal processing*,
2014.

</div>

<div id="ref-mysore10">

\[282\] G. J. Mysore, P. Smaragdis, and B. Raj, “Non-negative hidden
Markov modeling of audio with application to source separation,” in *9th
international conference on latent variable analysis and signal
separation*, 2010.

</div>

<div id="ref-qian17">

\[283\] K. Qian, Y. Zhang, S. Chang, X. Yang, D. Florêncio, and M.
Hasegawa-Johnson, “Speech enhancement using bayesian wavenet,” *Proc.
Interspeech 2017*, pp. 2013–2017, 2017.

</div>

<div id="ref-deng14">

\[284\] L. Deng and D. Yu, “Deep learning: Methods and applications,”
*Foundations and Trends in Signal Processing*, vol. 7, nos. 3-4, pp.
197–387, Jun. 2014.

</div>

<div id="ref-lecun15">

\[285\] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*,
vol. 521, pp. 436–444, May 2015.

</div>

<div id="ref-goodfellow16">

\[286\] I. Goodfellow, Y. Bengio, and A. Courville, *Deep learning*. MIT
Press, 2016.

</div>

<div id="ref-robbins51">

\[287\] H. Robbins and S. Monro, “A stochastic approximation method,”
*Annals of Mathematical Statistics*, vol. 22, no. 3, pp. 400–407, Sep.
1951.

</div>

<div id="ref-rumelhart862">

\[288\] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning
representations by back-propagating errors,” *Nature*, vol. 323, pp.
533–536, Oct. 1986.

</div>

<div id="ref-hermans13">

\[289\] M. Hermans and B. Schrauwen, “Training and analysing deep
recurrent neural networks,” in *26th international conference on neural
information processing systems*, 2013.

</div>

<div id="ref-pascanu14">

\[290\] R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio, “How to
construct deep recurrent neural networks,” in *International conference
on learning representations*, 2014.

</div>

<div id="ref-huang15">

\[291\] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,
“Joint optimization of masks and deep recurrent neural networks for
monaural source separation,” *IEEE/ACM Transactions on Audio, Speech,
and Language Processing*, vol. 23, 2015.

</div>

<div id="ref-huang142">

\[292\] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,
“Deep learning for monaural speech separation,” in *IEEE international
conference on acoustics, speech and signal processing*, 2014.

</div>

<div id="ref-uhlich15">

\[293\] S. Uhlich, F. Giron, and Y. Mitsufuji, “Deep neural network
based instrument extraction from music,” in *IEEE international
conference on acoustics, speech and signal processing*, 2015.

</div>

<div id="ref-uhlich17">

\[294\] S. Uhlich *et al.*, “Improving music source separation based on
deep neural networks through data augmentation and network blending,” in
*IEEE international conference on acoustics, speech and signal
processing*, 2017.

</div>

<div id="ref-simpson15">

\[295\] A. J. R. Simpson, G. Roma, and M. D. Plumbley, “Deep karaoke:
Extracting vocals from musical mixtures using a convolutional deep
neural network,” in *12th international conference on latent variable
analysis and signal separation*, 2015.

</div>

<div id="ref-schlueter16">

\[296\] J. Schlüter, “Learning to pinpoint singing voice from weakly
labeled examples,” in *17th international society for music information
retrieval conference*, 2016.

</div>

<div id="ref-chandna17">

\[297\] P. Chandna, M. Miron, J. Janer, and E. Gómez, “Monoaural audio
source separation using deep convolutional neural networks,” in *13th
international conference on latent variable analysis and signal
separation*, 2017.

</div>

<div id="ref-mimilakis16">

\[298\] S. I. Mimilakis, E. Cano, J. Abeßer, and G. Schuller, “New
sonorities for jazz recordings: Separation and mixing using deep neural
networks,” in *2nd aes workshop on intelligent music production*, 2016.

</div>

<div id="ref-mimilakis17">

\[299\] S. I. Mimilakis, K. Drossos, T. Virtanen, and G. Schuller, “A
recurrent encoder-decoder approach with skip-filtering connections for
monaural singing voice separation,” in *IEEE international workshop on
machine learning for signal processing*, 2017.

</div>

<div id="ref-mimilakis172">

\[300\] S. I. Mimilakis, K. Drossos, J. F. Santos, G. Schuller, T.
Virtanen, and Y. Bengio, “Monaural singing voice separation with
skip-filtering connections and recurrent inference of time-frequency
mask,” in *IEEE international conference on acoustics, speech and signal
processing*, 2018.

</div>

<div id="ref-jansson17">

\[301\] A. Jansson, E. Humphrey, N. Montecchio, R. Bittner, A. Kumar,
and T. Weyde, “Singing voice separation with deep U-Net convolutional
networks,” in *18th international society for music information
retrieval conferenceng*, 2017.

</div>

<div id="ref-takahashi17">

\[302\] N. Takahashi and Y. Mitsufuji, “Multi-scale multi-band densenets
for audio source separation,” in *IEEE workshop on applications of
signal processing to audio and acoustics*, 2017.

</div>

<div id="ref-hershey16">

\[303\] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep
clustering: Discriminative embeddings for segmentation and separation,”
in *IEEE international conference on acoustics, speech and signal
processing*, 2016.

</div>

<div id="ref-isik16">

\[304\] Y. Isik, J. L. Roux, Z. Chen, S. Watanabe, and J. R. Hershey,
“Single-channel multispeaker separation using deep clustering,” in
*Interspeech*, 2016.

</div>

<div id="ref-luo17">

\[305\] Y. Luo, Z. Chen, J. R. Hershey, J. L. Roux, and N. Mesgarani,
“Deep clustering and conventional networks for music separation:
Stronger together,” in *IEEE international conference on acoustics,
speech and signal processing*, 2017.

</div>

<div id="ref-kim15">

\[306\] M. Kim and P. Smaragdis, “Adaptive denoising autoencoders: A
fine-tuning scheme to learn from test mixtures,” in *12th international
conference on latent variable analysis and signal separation*, 2015.

</div>

<div id="ref-vincentp10">

\[307\] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A.
Manzagol, “Stacked denoising autoencoders: Learning useful
representations in a deep network with a local denoising criterion,”
*Journal of Machine Learning Research*, vol. 11, pp. 3371–3408, Dec.
2010.

</div>

<div id="ref-grais16">

\[308\] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,
“Single channel audio source separation using deep neural network
ensembles,” in *140th aes convention*, 2016.

</div>

<div id="ref-grais162">

\[309\] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,
“Combining mask estimates for single channel audio source separation
using deep neural networks,” in *Interspeech*, 2016.

</div>

<div id="ref-grais17">

\[310\] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,
“Discriminative enhancement for single channel audio source separation
using deep neural networks,” in *13th international conference on latent
variable analysis and signal separation*, 2017.

</div>

<div id="ref-grais172">

\[311\] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,
“Two-stage single-channel audio source separation using deep neural
networks,” *IEEE/ACM Transactions on Audio, Speech, and Language
Processing*, vol. 25, no. 9, pp. 1773–1783, Sep. 2017.

</div>

<div id="ref-nie15">

\[312\] S. Nie *et al.*, “Joint optimization of recurrent networks
exploiting source auto-regression for source separation,” in
*Interspeech*, 2015.

</div>

<div id="ref-sebastian16">

\[313\] J. Sebastian and H. A. Murthy, “Group delay based music source
separation using deep recurrent neural networks,” in *International
conference on signal processing and communications*, 2016.

</div>

<div id="ref-yegnanarayana91">

\[314\] B. Yegnanarayana, H. A. Murthy, and V. R. Ramachandran,
“Processing of noisy speech using modified group delay functions,” in
*IEEE international conference on acoustics, speech and signal
processing*, 1991.

</div>

<div id="ref-fan16">

\[315\] Z.-C. Fan, J.-S. R. Jang, and C.-L. Lu, “Singing voice
separation and pitch extraction from monaural polyphonic audio music via
DNN and adaptive pitch tracking,” in *IEEE international conference on
multimedia big data*, 2016.

</div>

<div id="ref-avendano03">

\[316\] C. Avendano, “Frequency-domain source identification and
manipulation in stereo mixes for enhancement, suppression and re-panning
applications,” in *IEEE workshop on applications of signal processing to
audio and acoustics*, 2003.

</div>

<div id="ref-avendano02">

\[317\] C. Avendano and J.-M. Jot, “Frequency domain techniques for
stereo to multichannel upmix,” in *AES 22nd international conference*,
2002.

</div>

<div id="ref-barry04">

\[318\] D. Barry, B. Lawlor, and E. Coyle, “Sound source separation:
Azimuth discrimination and resynthesis,” in *7th international
conference on digital audio effects*, 2004.

</div>

<div id="ref-vinyes06">

\[319\] M. Vinyes, J. Bonada, and A. Loscos, “Demixing commercial music
productions via human-assisted time-frequency masking,” in *120th aes
convention*, 2006.

</div>

<div id="ref-cobos082">

\[320\] M. Cobos and J. J. López, “Stereo audio source separation based
on time-frequency masking and multilevel thresholding,” *Digital Signal
Processing*, vol. 18, no. 6, pp. 960–976, Nov. 2008.

</div>

<div id="ref-yilmaz04">

\[321\] Ö. Yilmaz and S. Rickard, “Blind separation of speech mixtures
via time-frequency masking,” *IEEE Transactions on Signal Processing*,
vol. 52, no. 7, pp. 1830–1847, Jul. 2004.

</div>

<div id="ref-otsu79">

\[322\] N. Otsu, “A threshold selection method from gray-level
histograms,” *IEEE Transactions on Systems, Man, and Cybernetics*, vol.
9, no. 1, pp. 62–66, Jan. 1979.

</div>

<div id="ref-sofianos10">

\[323\] S. Sofianos, A. Ariyaeeinia, and R. Polfreman, “Towards
effective singing voice extraction from stereophonic recordings,” in
*IEEE international conference on acoustics, speech and signal
processing*, 2010.

</div>

<div id="ref-sofianos102">

\[324\] S. Sofianos, A. Ariyaeeinia, and R. Polfreman, “Singing voice
separation based on non-vocal independent component subtraction,” in
*13th international conference on digital audio effects*, 2010.

</div>

<div id="ref-sofianos12">

\[325\] S. Sofianos, A. Ariyaeeinia, R. Polfreman, and R. Sotudeh,
“H-semantics: A hybrid approach to singing voice separation,” *Journal
of the Audio Engineering Society*, vol. 60, no. 10, pp. 831–841, Oct.
2012.

</div>

<div id="ref-kim11">

\[326\] M. Kim, S. Beack, K. Choi, and K. Kang, “Gaussian mixture model
for singing voice separation from stereophonic music,” in *AES 43rd
conference*, 2011.

</div>

<div id="ref-cobos08">

\[327\] M. Cobos and J. J. López, “Singing voice separation combining
panning information and pitch tracking,” in *AES 124th convention*,
2008.

</div>

<div id="ref-fitzgerald13">

\[328\] D. FitzGerald, “Stereo vocal extraction using ADRess and nearest
neighbours median filtering,” in *16th international conference on
digital audio effects*, 2013.

</div>

<div id="ref-fitzgerald132">

\[329\] D. FitzGerald and R. Jaiswal, “Improved stereo instrumental
track recovery using median nearest-neighbour inpainting,” in *24th iet
irish signals and systems conference*, 2013.

</div>

<div id="ref-alder12">

\[330\] A. Adler, V. Emiya, M. G. Jafari, M. Elad, R. Gribonval, and M.
D. Plumbley, “Audio inpainting,” *IEEE Transactions on Audio, Speech,
and Language Processing*, vol. 20, no. 3, pp. 922–932, Mar. 2012.

</div>

<div id="ref-ozerov09">

\[331\] A. Ozerov and C. Févotte, “Multichannel nonnegative matrix
factorization in convolutive mixtures with application to blind audio
source separation,” in *IEEE international conference on acoustics,
speech and signal processing*, 2009.

</div>

<div id="ref-ozerov10">

\[332\] A. Ozerov and C. Févotte, “Multichannel nonnegative matrix
factorization in convolutive mixtures for audio source separation,”
*IEEE Transactions on Audio, Speech, and Language Processing*, vol. 18,
no. 3, pp. 550–563, Mar. 2010.

</div>

<div id="ref-ozerov11">

\[333\] A. Ozerov, C. Févotte, R. Blouet, and J.-L. Durrieu,
“Multichannel nonnegative tensor factorization with structured
constraints for user-guided audio source separation,” in *IEEE
international conference on acoustics, speech and signal processing*,
2011.

</div>

<div id="ref-liutkus10">

\[334\] A. Liutkus, R. Badeau, and G. Richard, “Informed source
separation using latent components,” in *9th international conference on
latent variable analysis and signal separation*, 2010.

</div>

<div id="ref-fevotte10">

\[335\] C. Févotte and A. Ozerov, “Notes on nonnegative tensor
factorization of the spectrogram for audio source separation:
Statistical insights and towards self-clustering of the spatial cues,”
in *7th international symposium on computer music modeling and
retrieval*, 2010.

</div>

<div id="ref-ozerov14">

\[336\] A. Ozerov, N. Duong, and L. Chevallier, “On monotonicity of
multiplicative update rules for weighted nonnegative tensor
factorization,” in *International symposium on nonlinear theory and its
applications*, 2014.

</div>

<div id="ref-sawada11">

\[337\] H. Sawada, H. Kameoka, S. Araki, and N. Ueda, “New formulations
and efficient algorithms for multichannel NMF,” in *IEEE workshop on
applications of signal processing to audio and acoustics*, 2011.

</div>

<div id="ref-sawada12">

\[338\] H. Sawada, H. Kameoka, S. Araki, and N. Ueda, “Efficient
algorithms for multichannel extensions of Itakura-Saito nonnegative
matrix factorization,” in *IEEE international conference on acoustics,
speech and signal processing*, 2012.

</div>

<div id="ref-sawada13">

\[339\] H. Sawada, H. Kameoka, S. Araki, and N. Ueda, “Multichannel
extensions of non-negative matrix factorization with complex-valued
data,” *IEEE Transactions on Audio, Speech, and Language Processing*,
vol. 21, no. 5, pp. 971–982, May 2013.

</div>

<div id="ref-sivasankaran15">

\[340\] S. Sivasankaran *et al.*, “Robust ASR using neural network based
speech enhancement and feature simulation,” in *IEEE automatic speech
recognition and understanding workshop*, 2015.

</div>

<div id="ref-nugraha162">

\[341\] A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichannel audio
source separation with deep neural networks,” *IEEE/ACM Transactions on
Audio, Speech, and Language Processing*, vol. 24, no. 9, pp. 1652–1664,
Sep. 2016.

</div>

<div id="ref-nugraha15">

\[342\] A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichannel audio
source separation with deep neural networks,” Inria, 2015.

</div>

<div id="ref-nugraha16">

\[343\] A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichannel music
separation with deep neural networks,” in *24th european signal
processing conference*, 2016.

</div>

<div id="ref-duong10">

\[344\] N. Q. K. Duong, E. Vincent, and R. Gribonval, “Under-determined
reverberant audio source separation using a full-rank spatial covariance
model,” *IEEE Transactions on Audio, Speech, and Language Processing*,
vol. 18, no. 7, pp. 1830–1840, Sep. 2010.

</div>

<div id="ref-ozerov112">

\[345\] A. Ozerov, A. Liutkus, R. Badeau, and G. Richard, “Informed
source separation: Source coding meets source separation,” in *IEEE
workshop on applications of signal processing to audio and acoustics*,
2011.

</div>

<div id="ref-zwicker13">

\[346\] E. Zwicker and H. Fastl, *Psychoacoustics: Facts and models*.
Springer-Verlag Berlin Heidelberg, 2013.

</div>

<div id="ref-rix01">

\[347\] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra,
“Perceptual evaluation of speech quality (PESQ)-a new method for
speech quality assessment of telephone networks and codecs,” in *IEEE
international conference on acoustics, speech and signal processing*,
2001.

</div>

<div id="ref-wang09">

\[348\] Z. Wang and A. C. Bovik, “Mean squared error: Love it or leave
it? A new look at signal fidelity measures,” *IEEE Signal Processing
Magazine*, vol. 26, no. 1, pp. 98–117, Jan. 2009.

</div>

<div id="ref-barker15">

\[349\] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, “The third
‘CHiME’ speech separation and recognition challenge: Dataset, task and
baselines,” in *IEEE workshop on automatic speech recognition and
understanding*, 2015.

</div>

<div id="ref-recommendation2001MUSHRA">

\[350\] I. Recommendation, “Bs. 1534-1. method for the subjective
assessment of intermediate sound quality (MUSHRA),” *International
Telecommunications Union, Geneva*, 2001.

</div>

<div id="ref-vincent062">

\[351\] E. Vincent, M. Jafari, and M. Plumbley, “Preliminary guidelines
for subjective evaluation of audio source separation algorithms,” in
*ICA research network international workshop*, 2006.

</div>

<div id="ref-cano11">

\[352\] E. Cano, C. Dittmar, and G. Schuller, “Influence of phase,
magnitude and location of harmonic components in the perceived quality
of extracted solo signals,” in *AES 42nd conference on semantic audio*,
2011.

</div>

<div id="ref-fevotte05">

\[353\] C. Févotte, R. Gribonval, and E. Vinvent, “BSS\_EVAL toolbox
user guide - revision 2.0,” IRISA, 2005.

</div>

<div id="ref-vincent06">

\[354\] E. Vincent, R. Gribonval, and C. Févotte, “Performance
measurement in blind audio source separation,” *IEEE Transactions on
Audio, Speech, and Language Processing*, vol. 14, no. 4, pp. 1462–1469,
Jul. 2006.

</div>

<div id="ref-fox07">

\[355\] B. Fox, A. Sabin, B. Pardo, and A. Zopf, “Modeling perceptual
similarity of audio signals for blind source separation evaluation,” in
*7th international conference on latent variable analysis and signal
separation*, 2007.

</div>

<div id="ref-fox072">

\[356\] B. Fox and B. Pardo, “Towards a model of perceived quality of
blind audio source separation,” in *IEEE international conference on
multimedia and expo*, 2007.

</div>

<div id="ref-kornycky08">

\[357\] J. Kornycky, B. Gunel, and A. Kondoz, “Comparison of subjective
and objective evaluation methods for audio source separation,” *Journal
of the Acoustical Society of America*, vol. 4, no. 1, 2008.

</div>

<div id="ref-emiya10">

\[358\] V. Emiya, E. Vincent, N. Harlander, and V. Hohmann,
“Multi-criteria subjective and objective evaluation of audio source
separation,” in *38th international aes conference*, 2010.

</div>

<div id="ref-emiya11">

\[359\] V. Emiya, E. Vincent, N. Harlander, and V. Hohmann, “Subjective
and objective quality assessment of audio source separation,” *IEEE
Transactions on Audio, Speech, and Language Processing*, vol. 19, no. 7,
pp. 2046–2057, Sep. 2011.

</div>

<div id="ref-vincent122">

\[360\] E. Vincent, “Improved perceptual metrics for the evaluation of
audio source separation,” in *10th international conference on latent
variable analysis and signal separation*, 2012.

</div>

<div id="ref-cartwright16">

\[361\] M. Cartwright, B. Pardo, G. J. Mysore, and M. Hoffman, “Fast and
easy crowdsourced perceptual audio evaluation,” in *IEEE international
conference on acoustics, speech and signal processing*, 2016.

</div>

<div id="ref-gupta15">

\[362\] U. Gupta, E. Moore, and A. Lerch, “On the perceptual relevance
of objective source separation measures for singing voice separation,”
in *IEEE workshop on applications of signal processing to audio and
acoustics*, 2005.

</div>

<div id="ref-stoter16">

\[363\] F.-R. Stöter, A. Liutkus, R. Badeau, B. Edler, and P. Magron,
“Common fate model for unison source separation,” in *IEEE
international conference on acoustics, speech and signal processing*,
2016.

</div>

<div id="ref-roma16">

\[364\] G. Roma, E. M. Grais, A. J. Simpson, I. Sobieraj, and M. D.
Plumbley, “Untwist: A new toolbox for audio source separation,” in *17th
international society on music information retrieval conference*, 2016.

</div>
</div>
